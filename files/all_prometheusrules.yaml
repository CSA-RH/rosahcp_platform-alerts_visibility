apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: alertmanager-main-rules
spec:
  groups:
  - name: alertmanager.rules
    rules:
    - alert: AlertmanagerFailedReload
      annotations:
        description: Configuration has failed to load for {{ $labels.namespace }}/{{
          $labels.pod}}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerFailedReload.md
        summary: Reloading an Alertmanager configuration has failed.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(alertmanager_config_last_reload_successful{job=~"alertmanager-main|alertmanager-user-workload"}[5m]) == 0
      for: 10m
      labels:
        severity: critical
    - alert: AlertmanagerMembersInconsistent
      annotations:
        description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} has only
          found {{ $value }} members of the {{$labels.job}} cluster.
        summary: A member of an Alertmanager cluster has not found all other cluster
          members.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          max_over_time(alertmanager_cluster_members{job=~"alertmanager-main|alertmanager-user-workload"}[5m])
        < on (namespace,service) group_left
          count by (namespace,service) (max_over_time(alertmanager_cluster_members{job=~"alertmanager-main|alertmanager-user-workload"}[5m]))
      for: 15m
      labels:
        severity: warning
    - alert: AlertmanagerFailedToSendAlerts
      annotations:
        description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} failed
          to send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration
          }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerFailedToSendAlerts.md
        summary: An Alertmanager instance failed to send notifications.
      expr: |
        (
          rate(alertmanager_notifications_failed_total{job=~"alertmanager-main|alertmanager-user-workload"}[5m])
        /
          ignoring (reason) group_left rate(alertmanager_notifications_total{job=~"alertmanager-main|alertmanager-user-workload"}[5m])
        )
        > 0.01
      for: 5m
      labels:
        severity: warning
    - alert: AlertmanagerClusterFailedToSendAlerts
      annotations:
        description: The minimum notification failure rate to {{ $labels.integration
          }} sent from any instance in the {{$labels.job}} cluster is {{ $value |
          humanizePercentage }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerClusterFailedToSendAlerts.md
        summary: All Alertmanager instances in a cluster failed to send notifications
          to a critical integration.
      expr: |
        min by (namespace,service, integration) (
          rate(alertmanager_notifications_failed_total{job=~"alertmanager-main|alertmanager-user-workload", integration=~`.*`}[5m])
        /
          ignoring (reason) group_left rate(alertmanager_notifications_total{job=~"alertmanager-main|alertmanager-user-workload", integration=~`.*`}[5m])
        )
        > 0.01
      for: 5m
      labels:
        severity: warning
    - alert: AlertmanagerConfigInconsistent
      annotations:
        description: Alertmanager instances within the {{$labels.job}} cluster have
          different configurations.
        summary: Alertmanager instances within the same cluster have different configurations.
      expr: |
        count by (namespace,service) (
          count_values by (namespace,service) ("config_hash", alertmanager_config_hash{job=~"alertmanager-main|alertmanager-user-workload"})
        )
        != 1
      for: 20m
      labels:
        severity: warning
    - alert: AlertmanagerClusterDown
      annotations:
        description: '{{ $value | humanizePercentage }} of Alertmanager instances
          within the {{$labels.job}} cluster have been up for less than half of the
          last 5m.'
        summary: Half or more of the Alertmanager instances within the same cluster
          are down.
      expr: |
        (
          count by (namespace,service) (
            avg_over_time(up{job=~"alertmanager-main|alertmanager-user-workload"}[5m]) < 0.5
          )
        /
          count by (namespace,service) (
            up{job=~"alertmanager-main|alertmanager-user-workload"}
          )
        )
        >= 0.5
      for: 5m
      labels:
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: api-usage
spec:
  groups:
  - name: pre-release-lifecycle
    rules:
    - alert: APIRemovedInNextReleaseInUse
      annotations:
        description: Deprecated API that will be removed in the next version is being
          used. Removing the workload that is using the {{ $labels.group }}.{{ $labels.version
          }}/{{ $labels.resource }} API might be necessary for a successful upgrade
          to the next cluster version. Refer to `oc get apirequestcounts {{ $labels.resource
          }}.{{ $labels.version }}.{{ $labels.group }} -o yaml` to identify the workload.
        summary: Deprecated API that will be removed in the next version is being
          used.
      expr: |
        group(apiserver_requested_deprecated_apis{removed_release="1.24"}) by (group,version,resource) and (sum by(group,version,resource) (rate(apiserver_request_total{system_client!="kube-controller-manager",system_client!="cluster-policy-controller"}[4h]))) > 0
      for: 1h
      labels:
        namespace: openshift-kube-apiserver
        severity: info
    - alert: APIRemovedInNextEUSReleaseInUse
      annotations:
        description: Deprecated API that will be removed in the next EUS version is
          being used. Removing the workload that is using the {{ $labels.group }}.{{
          $labels.version }}/{{ $labels.resource }} API might be necessary for a successful
          upgrade to the next EUS cluster version. Refer to `oc get apirequestcounts
          {{ $labels.resource }}.{{ $labels.version }}.{{ $labels.group }} -o yaml`
          to identify the workload.
        summary: Deprecated API that will be removed in the next EUS version is being
          used.
      expr: |
        group(apiserver_requested_deprecated_apis{removed_release=~"1\\.2[45]"}) by (group,version,resource) and (sum by(group,version,resource) (rate(apiserver_request_total{system_client!="kube-controller-manager",system_client!="cluster-policy-controller"}[4h]))) > 0
      for: 1h
      labels:
        namespace: openshift-kube-apiserver
        severity: info
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: cluster-monitoring-operator-prometheus-rules
spec:
  groups:
  - name: openshift-general.rules
    rules:
    - alert: TargetDown
      annotations:
        description: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service
          }} targets in {{ $labels.namespace }} namespace have been unreachable for
          more than 15 minutes. This may be a symptom of network connectivity issues,
          down nodes, or failures within these components. Assess the health of the
          infrastructure and nodes running these targets and then contact support.'
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/TargetDown.md
        summary: Some targets were not reachable from the monitoring server for an
          extended period of time.
      expr: |
        100 * ((
          1 - sum   by (job, namespace, service) (up and on(namespace, pod) kube_pod_info) /
              count by (job, namespace, service) (up and on(namespace, pod) kube_pod_info)
        ) or (
          count by (job, namespace, service) (up == 0) /
          count by (job, namespace, service) (up)
        )) > 10
      for: 15m
      labels:
        severity: warning
  - name: openshift-kubernetes.rules
    rules:
    - expr: sum(rate(container_cpu_usage_seconds_total{container="",pod!=""}[5m]))
        BY (pod, namespace)
      record: pod:container_cpu_usage:sum
    - expr: sum(container_fs_usage_bytes{pod!=""}) BY (pod, namespace)
      record: pod:container_fs_usage_bytes:sum
    - expr: sum(container_memory_usage_bytes{container!=""}) BY (namespace)
      record: namespace:container_memory_usage_bytes:sum
    - expr: sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[5m]))
        BY (namespace)
      record: namespace:container_cpu_usage:sum
    - expr: sum(container_memory_usage_bytes{container="",pod!=""}) BY (cluster) /
        sum(machine_memory_bytes) BY (cluster)
      record: cluster:memory_usage:ratio
    - expr: sum(container_spec_cpu_shares{container="",pod!=""}) / 1000 / sum(machine_cpu_cores)
      record: cluster:container_spec_cpu_shares:ratio
    - expr: sum(rate(container_cpu_usage_seconds_total{container="",pod!=""}[5m]))
        / sum(machine_cpu_cores)
      record: cluster:container_cpu_usage:ratio
    - expr: |
        sum by(namespace,pod, interface) (irate(container_network_receive_bytes_total{pod!=""}[5m]))
        +
        on(namespace,pod, interface) group_left(network_name) topk by(namespace,pod, interface) (1, pod_network_name_info)
      record: pod_interface_network:container_network_receive_bytes:irate5m
    - expr: |
        sum by(namespace,pod, interface) (irate(container_network_transmit_bytes_total{pod!=""}[5m]))
        +
        on(namespace,pod, interface) group_left(network_name) topk by(namespace,pod, interface) (1, pod_network_name_info)
      record: pod_interface_network:container_network_transmit_bytes_total:irate5m
    - expr: max without(endpoint, instance, job, pod, service) (kube_node_labels and
        on(node) kube_node_role{role="master"})
      labels:
        label_node_role_kubernetes_io: master
        label_node_role_kubernetes_io_master: "true"
      record: cluster:master_nodes
    - expr: max without(endpoint, instance, job, pod, service) (kube_node_labels and
        on(node) kube_node_role{role="infra"})
      labels:
        label_node_role_kubernetes_io_infra: "true"
      record: cluster:infra_nodes
    - expr: max without(endpoint, instance, job, pod, service) (cluster:master_nodes
        and on(node) cluster:infra_nodes)
      labels:
        label_node_role_kubernetes_io_infra: "true"
        label_node_role_kubernetes_io_master: "true"
      record: cluster:master_infra_nodes
    - expr: cluster:master_infra_nodes or on (node) cluster:master_nodes or on (node)
        cluster:infra_nodes or on (node) max without(endpoint, instance, job, pod,
        service) (kube_node_labels)
      record: cluster:nodes_roles
    - expr: kube_node_labels and on(node) (sum(label_replace(node_cpu_info, "node",
        "$1", "instance", "(.*)")) by (node, package, core) == 2)
      labels:
        label_node_hyperthread_enabled: "true"
      record: cluster:hyperthread_enabled_nodes
    - expr: count(sum(virt_platform) by (instance, type, system_manufacturer, system_product_name,
        baseboard_manufacturer, baseboard_product_name)) by (type, system_manufacturer,
        system_product_name, baseboard_manufacturer, baseboard_product_name)
      record: cluster:virt_platform_nodes:sum
    - expr: |
        sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id) (
          (
            cluster:master_nodes
            * on(node) group_left() max by(node)
            (
              kube_node_status_capacity{resource="cpu",unit="core"}
            )
          )
          or on(node) (
            label_replace(cluster:infra_nodes, "label_node_role_kubernetes_io", "infra", "", "")
            * on(node) group_left() max by(node)
            (
              kube_node_status_capacity{resource="cpu",unit="core"}
            )
          )
          or on(node) (
            max without(endpoint, instance, job, pod, service)
            (
              kube_node_labels
            ) * on(node) group_left() max by(node)
            (
              kube_node_status_capacity{resource="cpu",unit="core"}
            )
          )
        )
      record: cluster:capacity_cpu_cores:sum
    - expr: |
        clamp_max(
          label_replace(
            sum by(instance, package, core) (
              node_cpu_info{core!="",package!=""}
              or
              # Assume core = cpu and package = 0 for platforms that don't expose core/package labels.
              label_replace(label_join(node_cpu_info{core="",package=""}, "core", "", "cpu"), "package", "0", "package", "")
            ) > 1,
            "label_node_hyperthread_enabled",
            "true",
            "instance",
            "(.*)"
          ) or on (instance, package)
          label_replace(
            sum by(instance, package, core) (
              label_replace(node_cpu_info{core!="",package!=""}
              or
              # Assume core = cpu and package = 0 for platforms that don't expose core/package labels.
              label_join(node_cpu_info{core="",package=""}, "core", "", "cpu"), "package", "0", "package", "")
            ) <= 1,
            "label_node_hyperthread_enabled",
            "false",
            "instance",
            "(.*)"
          ),
          1
        )
      record: cluster:cpu_core_hyperthreading
    - expr: |
        topk by(node) (1, cluster:nodes_roles) * on (node)
          group_right( label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_node_openshift_io_os_id, label_kubernetes_io_arch,
                       label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra)
        label_replace( cluster:cpu_core_hyperthreading, "node", "$1", "instance", "(.*)" )
      record: cluster:cpu_core_node_labels
    - expr: count(cluster:cpu_core_node_labels) by (label_beta_kubernetes_io_instance_type,
        label_node_hyperthread_enabled)
      record: cluster:capacity_cpu_cores_hyperthread_enabled:sum
    - expr: |
        sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io)
        (
          (
            cluster:master_nodes
            * on(node) group_left() max by(node)
            (
              kube_node_status_capacity{resource="memory",unit="byte"}
            )
          )
          or on(node)
          (
            max without(endpoint, instance, job, pod, service)
            (
              kube_node_labels
            )
            * on(node) group_left() max by(node)
            (
              kube_node_status_capacity{resource="memory",unit="byte"}
            )
          )
        )
      record: cluster:capacity_memory_bytes:sum
    - expr: sum(1 - rate(node_cpu_seconds_total{mode="idle"}[2m]) * on(namespace,
        pod) group_left(node) node_namespace_pod:kube_pod_info:{pod=~"node-exporter.+"})
      record: cluster:cpu_usage_cores:sum
    - expr: sum(node_memory_MemTotal_bytes{job="node-exporter"} - node_memory_MemAvailable_bytes{job="node-exporter"})
      record: cluster:memory_usage_bytes:sum
    - expr: sum(rate(container_cpu_usage_seconds_total{namespace!~"openshift-.+",pod!="",container=""}[5m]))
      record: workload:cpu_usage_cores:sum
    - expr: cluster:cpu_usage_cores:sum - workload:cpu_usage_cores:sum
      record: openshift:cpu_usage_cores:sum
    - expr: sum(container_memory_working_set_bytes{namespace!~"openshift-.+",pod!="",container=""})
      record: workload:memory_usage_bytes:sum
    - expr: cluster:memory_usage_bytes:sum - workload:memory_usage_bytes:sum
      record: openshift:memory_usage_bytes:sum
    - expr: sum(cluster:master_nodes or on(node) kube_node_labels ) BY (label_beta_kubernetes_io_instance_type,
        label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id)
      record: cluster:node_instance_type_count:sum
    - expr: |
        sum by(provisioner) (
          topk by (namespace, persistentvolumeclaim) (
            1, kube_persistentvolumeclaim_resource_requests_storage_bytes
          ) * on(namespace, persistentvolumeclaim) group_right()
          topk by(namespace, persistentvolumeclaim) (
            1, kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) topk by(storageclass) (1, max by(storageclass, provisioner) (kube_storageclass_info))
          )
        )
      record: cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum
    - expr: (sum(node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io_master="",label_node_role_kubernetes_io_infra=""}
        or absent(__does_not_exist__)*0)) + ((sum(node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io_master="true"}
        or absent(__does_not_exist__)*0) * ((max(cluster_master_schedulable == 1)*0+1)
        or (absent(cluster_master_schedulable == 1)*0))))
      record: workload:capacity_physical_cpu_cores:sum
    - expr: min_over_time(workload:capacity_physical_cpu_cores:sum[5m:15s])
      record: cluster:usage:workload:capacity_physical_cpu_cores:min:5m
    - expr: max_over_time(workload:capacity_physical_cpu_cores:sum[5m:15s])
      record: cluster:usage:workload:capacity_physical_cpu_cores:max:5m
    - expr: |
        sum  by (provisioner) (
          topk by (namespace, persistentvolumeclaim) (
            1, kubelet_volume_stats_used_bytes
          ) * on (namespace,persistentvolumeclaim) group_right()
          topk by (namespace, persistentvolumeclaim) (
            1, kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) topk by(storageclass) (1, max by(storageclass, provisioner) (kube_storageclass_info))
          )
        )
      record: cluster:kubelet_volume_stats_used_bytes:provisioner:sum
    - expr: sum by (instance) (apiserver_storage_objects != -1)
      record: instance:etcd_object_counts:sum
    - expr: topk(500, max by(resource) (apiserver_storage_objects != -1))
      record: cluster:usage:resources:sum
    - expr: count(count (kube_pod_restart_policy{type!="Always",namespace!~"openshift-.+"})
        by (namespace,pod))
      record: cluster:usage:pods:terminal:workload:sum
    - expr: sum(max(kubelet_containers_per_pod_count_sum) by (instance))
      record: cluster:usage:containers:sum
    - expr: count(cluster:cpu_core_node_labels) by (label_kubernetes_io_arch, label_node_hyperthread_enabled,
        label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
      record: node_role_os_version_machine:cpu_capacity_cores:sum
    - expr: count(max(cluster:cpu_core_node_labels) by (node, package, label_beta_kubernetes_io_instance_type,
        label_node_hyperthread_enabled, label_node_role_kubernetes_io) ) by ( label_beta_kubernetes_io_instance_type,
        label_node_hyperthread_enabled, label_node_role_kubernetes_io)
      record: cluster:capacity_cpu_sockets_hyperthread_enabled:sum
    - expr: count (max(cluster:cpu_core_node_labels) by (node, package, label_kubernetes_io_arch,
        label_node_hyperthread_enabled, label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
        ) by (label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
      record: node_role_os_version_machine:cpu_capacity_sockets:sum
    - expr: max(alertmanager_integrations{namespace="openshift-monitoring"})
      record: cluster:alertmanager_integrations:max
    - expr: sum by(plugin_name, volume_mode)(pv_collector_total_pv_count{volume_plugin!~".*-e2e-.*"})
      record: cluster:kube_persistentvolume_plugin_type_counts:sum
    - expr: |
        sum(
          min by (node) (kube_node_status_condition{condition="Ready",status="true"})
            and
          max by (node) (kube_node_role{role="master"})
        ) == bool sum(kube_node_role{role="master"})
      record: cluster:control_plane:all_nodes_ready
    - expr: max by (profile) (cluster_monitoring_operator_collection_profile == 1)
      record: profile:cluster_monitoring_operator_collection_profile:max
    - alert: ClusterMonitoringOperatorReconciliationErrors
      annotations:
        description: Errors are occurring during reconciliation cycles. Inspect the
          cluster-monitoring-operator log for potential root causes.
        summary: Cluster Monitoring Operator is experiencing unexpected reconciliation
          errors.
      expr: max_over_time(cluster_monitoring_operator_last_reconciliation_successful[5m])
        == 0
      for: 1h
      labels:
        severity: warning
    - alert: ClusterMonitoringOperatorDeprecatedConfig
      annotations:
        description: The configuration field {{ $labels.field }} in {{ $labels.configmap
          }} was deprecated in {{ $labels.deprecation_version }} and has no effect.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/ClusterMonitoringOperatorDeprecatedConfig.md
        summary: Cluster Monitoring Operator is being used with deprecated configuration.
      expr: max by (configmap, field, deprecation_version) (cluster_monitoring_operator_deprecated_config_in_use)
        == 1
      for: 1h
      labels:
        severity: info
    - alert: AlertmanagerReceiversNotConfigured
      annotations:
        description: Alerts are not configured to be sent to a notification system,
          meaning that you may not be notified in a timely fashion when important
          failures occur. Check the OpenShift documentation to learn how to configure
          notifications with Alertmanager.
        summary: Receivers (notification integrations) are not configured on Alertmanager
      expr: cluster:alertmanager_integrations:max == 0
      for: 10m
      labels:
        namespace: openshift-monitoring
        severity: warning
    - alert: KubeDeploymentReplicasMismatch
      annotations:
        description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has
          not matched the expected number of replicas for longer than 15 minutes.
          This indicates that cluster infrastructure is unable to start or restart
          the necessary components. This most often occurs when one or more nodes
          are down or partioned from the cluster, or a fault occurs on the node that
          prevents the workload from starting. In rare cases this may indicate a new
          version of a cluster component cannot start due to a bug or configuration
          error. Assess the pods for this deployment to verify they are running on
          healthy nodes and then contact support.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeDeploymentReplicasMismatch.md
        summary: Deployment has not matched the expected number of replicas
      expr: |
        (((
          kube_deployment_spec_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            >
          kube_deployment_status_replicas_available{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
        ) and (
          changes(kube_deployment_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m])
            ==
          0
        )) * on() group_left cluster:control_plane:all_nodes_ready) > 0
      for: 15m
      labels:
        severity: warning
    - expr: avg_over_time((((count((max by (node) (up{job="kubelet",metrics_path="/metrics"}
        == 1) and max by (node) (kube_node_status_condition{condition="Ready",status="true"}
        == 1) and min by (node) (kube_node_spec_unschedulable == 0))) / scalar(count(min
        by (node) (kube_node_spec_unschedulable == 0))))))[5m:1s])
      record: cluster:usage:kube_schedulable_node_ready_reachable:avg5m
    - expr: avg_over_time((count(max by (node) (kube_node_status_condition{condition="Ready",status="true"}
        == 1)) / scalar(count(max by (node) (kube_node_status_condition{condition="Ready",status="true"}))))[5m:1s])
      record: cluster:usage:kube_node_ready:avg5m
    - expr: (max without (condition,container,endpoint,instance,job,service) (((kube_pod_status_ready{condition="false"}
        == 1)*0 or (kube_pod_status_ready{condition="true"} == 1)) * on(pod,namespace)
        group_left() group by (pod,namespace) (kube_pod_status_phase{phase=~"Running|Unknown|Pending"}
        == 1)))
      record: kube_running_pod_ready
    - expr: avg(kube_running_pod_ready{namespace=~"openshift-.*"})
      record: cluster:usage:openshift:kube_running_pod_ready:avg
    - expr: avg(kube_running_pod_ready{namespace!~"openshift-.*"})
      record: cluster:usage:workload:kube_running_pod_ready:avg
    - alert: KubePodNotScheduled
      annotations:
        description: |-
          Pod {{ $labels.namespace }}/{{ $labels.pod }} cannot be scheduled for more than 30 minutes.
          Check the details of the pod with the following command:
          oc describe -n {{ $labels.namespace }} pod {{ $labels.pod }}
        summary: Pod cannot be scheduled.
      expr: last_over_time(kube_pod_status_unschedulable{namespace=~"(openshift-.*|kube-.*|default)"}[5m])
        == 1
      for: 30m
      labels:
        severity: warning
  - interval: 30s
    name: kubernetes-recurring.rules
    rules:
    - expr: sum_over_time(workload:capacity_physical_cpu_cores:sum[30s:1s]) + ((cluster:usage:workload:capacity_physical_cpu_core_seconds
        offset 25s) or (absent(cluster:usage:workload:capacity_physical_cpu_core_seconds
        offset 25s)*0))
      record: cluster:usage:workload:capacity_physical_cpu_core_seconds
  - name: openshift-ingress.rules
    rules:
    - expr: sum by (code) (rate(haproxy_server_http_responses_total[5m]) > 0)
      record: code:cluster:ingress_http_request_count:rate5m:sum
    - expr: sum (rate(haproxy_frontend_bytes_in_total[5m]))
      record: cluster:usage:ingress_frontend_bytes_in:rate5m:sum
    - expr: sum (rate(haproxy_frontend_bytes_out_total[5m]))
      record: cluster:usage:ingress_frontend_bytes_out:rate5m:sum
    - expr: sum (haproxy_frontend_current_sessions)
      record: cluster:usage:ingress_frontend_connections:sum
    - expr: sum(max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{code!~"2xx|1xx|4xx|3xx",exported_namespace!~"openshift-.*"}[5m])
        > 0)) / sum (max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{exported_namespace!~"openshift-.*"}[5m])))
        or absent(__does_not_exist__)*0
      record: cluster:usage:workload:ingress_request_error:fraction5m
    - expr: sum (max without(service,endpoint,container,pod,job,namespace) (irate(haproxy_server_http_responses_total{exported_namespace!~"openshift-.*"}[5m])))
        or absent(__does_not_exist__)*0
      record: cluster:usage:workload:ingress_request_total:irate5m
    - expr: sum(max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{code!~"2xx|1xx|4xx|3xx",exported_namespace=~"openshift-.*"}[5m])
        > 0)) / sum (max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{exported_namespace=~"openshift-.*"}[5m])))
        or absent(__does_not_exist__)*0
      record: cluster:usage:openshift:ingress_request_error:fraction5m
    - expr: sum (max without(service,endpoint,container,pod,job,namespace) (irate(haproxy_server_http_responses_total{exported_namespace=~"openshift-.*"}[5m])))
        or absent(__does_not_exist__)*0
      record: cluster:usage:openshift:ingress_request_total:irate5m
    - expr: sum(ingress_controller_aws_nlb_active) or vector(0)
      record: cluster:ingress_controller_aws_nlb_active:sum
  - name: openshift-build.rules
    rules:
    - expr: sum by (strategy) (openshift_build_status_phase_total)
      record: openshift:build_by_strategy:sum
  - name: openshift-monitoring.rules
    rules:
    - expr: sum by (job,namespace) (max without(instance) (prometheus_tsdb_head_series{namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}))
      record: openshift:prometheus_tsdb_head_series:sum
    - expr: sum by(job,namespace) (max without(instance) (rate(prometheus_tsdb_head_samples_appended_total{namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[2m])))
      record: openshift:prometheus_tsdb_head_samples_appended_total:sum
    - expr: sum by (namespace) (max without(instance) (container_memory_working_set_bytes{namespace=~"openshift-monitoring|openshift-user-workload-monitoring",
        container=""}))
      record: monitoring:container_memory_working_set_bytes:sum
    - expr: topk(3, sum by(namespace, job)(sum_over_time(scrape_series_added[1h])))
      record: namespace_job:scrape_series_added:topk3_sum1h
    - expr: topk(3, max by(namespace, job) (topk by(namespace,job) (1, scrape_samples_post_metric_relabeling)))
      record: namespace_job:scrape_samples_post_metric_relabeling:topk3
    - expr: sum by(exported_service) (rate(haproxy_server_http_responses_total{exported_namespace="openshift-monitoring",
        exported_service=~"alertmanager-main|prometheus-k8s"}[5m]))
      record: monitoring:haproxy_server_http_responses_total:sum
    - expr: max by (cluster, namespace, workload, pod) (label_replace(label_replace(kube_pod_owner{job="kube-state-metrics",
        owner_kind="ReplicationController"},"replicationcontroller", "$1", "owner_name",
        "(.*)") * on(replicationcontroller, namespace) group_left(owner_name) topk
        by(replicationcontroller, namespace) (1, max by (replicationcontroller, namespace,
        owner_name) (kube_replicationcontroller_owner{job="kube-state-metrics"})),"workload",
        "$1", "owner_name", "(.*)"))
      labels:
        workload_type: deploymentconfig
      record: namespace_workload_pod:kube_pod_owner:relabel
  - name: openshift-etcd-telemetry.rules
    rules:
    - expr: sum by (instance) (etcd_mvcc_db_total_size_in_bytes{job="etcd"})
      record: instance:etcd_mvcc_db_total_size_in_bytes:sum
    - expr: histogram_quantile(0.99, sum by (instance, le) (rate(etcd_disk_wal_fsync_duration_seconds_bucket{job="etcd"}[5m])))
      labels:
        quantile: "0.99"
      record: instance:etcd_disk_wal_fsync_duration_seconds:histogram_quantile
    - expr: histogram_quantile(0.99, sum by (instance, le) (rate(etcd_network_peer_round_trip_time_seconds_bucket{job="etcd"}[5m])))
      labels:
        quantile: "0.99"
      record: instance:etcd_network_peer_round_trip_time_seconds:histogram_quantile
    - expr: sum by (instance) (etcd_mvcc_db_total_size_in_use_in_bytes{job="etcd"})
      record: instance:etcd_mvcc_db_total_size_in_use_in_bytes:sum
    - expr: histogram_quantile(0.99, sum by (instance, le) (rate(etcd_disk_backend_commit_duration_seconds_bucket{job="etcd"}[5m])))
      labels:
        quantile: "0.99"
      record: instance:etcd_disk_backend_commit_duration_seconds:histogram_quantile
  - name: openshift-sre.rules
    rules:
    - expr: sum(rate(apiserver_request_total{job="apiserver"}[10m])) BY (code)
      record: code:apiserver_request_total:rate:sum
  - name: apiserver-list-watch.rules
    rules:
    - expr: sum by(verb) (rate(apiserver_request_total{verb=~"LIST|WATCH",code=~"2.."}[5m]))
      record: apiserver_list_watch_request_success_total:rate:sum
  - name: general.rules
    rules:
    - alert: Watchdog
      annotations:
        description: |
          This is an alert meant to ensure that the entire alerting pipeline is functional.
          This alert is always firing, therefore it should always be firing in Alertmanager
          and always fire against a receiver. There are integrations with various notification
          mechanisms that send a notification when this alert is not firing. For example the
          "DeadMansSnitch" integration in PagerDuty.
        summary: An alert that should always be firing to certify that Alertmanager
          is working properly.
      expr: vector(1)
      labels:
        namespace: openshift-monitoring
        severity: none
  - name: node-network
    rules:
    - alert: NodeNetworkInterfaceFlapping
      annotations:
        description: Network interface "{{ $labels.device }}" changing its up status
          often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}
        summary: Network interface is often changing its status
      expr: |
        changes(node_network_up{job="node-exporter",device!~"veth.+|tunbr"}[2m]) > 2
      for: 2m
      labels:
        severity: warning
  - name: kube-prometheus-node-recording.rules
    rules:
    - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[3m]))
        BY (instance)
      record: instance:node_cpu:rate:sum
    - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
      record: instance:node_network_receive_bytes:rate:sum
    - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
      record: instance:node_network_transmit_bytes:rate:sum
    - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[5m]))
      record: cluster:node_cpu:sum_rate5m
    - expr: cluster:node_cpu:sum_rate5m / count(sum(node_cpu_seconds_total) BY (instance,
        cpu))
      record: cluster:node_cpu:ratio
  - name: kube-prometheus-general.rules
    rules:
    - expr: count without(instance, pod, node) (up == 1)
      record: count:up1
    - expr: count without(instance, pod, node) (up == 0)
      record: count:up0
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: cluster-monitoring-prometheus-rules
spec:
  groups:
  - name: openshift/console-operator
    rules:
    - expr: sum(console_auth_login_requests_total)
      record: cluster:console_auth_login_requests_total:sum
    - expr: sum(console_auth_login_successes_total) by (role)
      record: cluster:console_auth_login_successes_total:sum
    - expr: sum(console_auth_login_failures_total) by (reason)
      record: cluster:console_auth_login_failures_total:sum
    - expr: sum(console_auth_logout_requests_total) by (reason)
      record: cluster:console_auth_logout_requests_total:sum
    - expr: max(console_usage_users) by (role)
      record: cluster:console_usage_users:max
    - expr: max(console_plugins_info) by (name, state)
      record: cluster:console_plugins_info:max
    - expr: max(console_customization_perspectives_info) by (name, state)
      record: cluster:console_customization_perspectives_info:max
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: dns
spec:
  groups:
  - name: openshift-dns.rules
    rules:
    - alert: CoreDNSPanicking
      annotations:
        description: '{{ $value }} CoreDNS panics observed on {{ $labels.instance
          }}'
        summary: CoreDNS panic
      expr: increase(coredns_panics_total[10m]) > 0
      for: 5m
      labels:
        severity: warning
    - alert: CoreDNSHealthCheckSlow
      annotations:
        description: CoreDNS Health Checks are slowing down (instance {{ $labels.instance
          }})
        summary: CoreDNS health checks
      expr: histogram_quantile(.95, sum(rate(coredns_health_request_duration_seconds_bucket[5m]))
        by (instance, le)) > 10
      for: 5m
      labels:
        severity: warning
    - alert: CoreDNSErrorsHigh
      annotations:
        description: CoreDNS is returning SERVFAIL for {{ $value | humanizePercentage
          }} of requests.
        summary: CoreDNS serverfail
      expr: |
        (sum by(namespace) (rate(coredns_dns_responses_total{rcode="SERVFAIL"}[5m]))
          /
        sum by(namespace) (rate(coredns_dns_responses_total[5m])))
        > 0.01
      for: 5m
      labels:
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: image-registry-rules
spec:
  groups:
  - name: imageregistry.operations.rules
    rules:
    - expr: |
        label_replace(
          label_replace(
            sum by (operation) (imageregistry_request_duration_seconds_count{operation="BlobStore.ServeBlob"}), "operation", "get", "operation", "(.+)"
          ), "resource_type", "blob", "resource_type", ""
        )
      record: imageregistry:operations_count:sum
    - expr: |
        label_replace(
          label_replace(
            sum by (operation) (imageregistry_request_duration_seconds_count{operation="BlobStore.Create"}), "operation", "create", "operation", "(.+)"
          ), "resource_type", "blob", "resource_type", ""
        )
      record: imageregistry:operations_count:sum
    - expr: |
        label_replace(
          label_replace(
            sum by (operation) (imageregistry_request_duration_seconds_count{operation="ManifestService.Get"}), "operation", "get", "operation", "(.+)"
          ), "resource_type", "manifest", "resource_type", ""
        )
      record: imageregistry:operations_count:sum
    - expr: |
        label_replace(
          label_replace(
            sum by (operation) (imageregistry_request_duration_seconds_count{operation="ManifestService.Put"}), "operation", "create", "operation", "(.+)"
          ), "resource_type", "manifest", "resource_type", ""
        )
      record: imageregistry:operations_count:sum
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: imagestreams-rules
spec:
  groups:
  - name: imagestreams.rules
    rules:
    - expr: sum by (location, source) (image_registry_image_stream_tags_total)
      record: imageregistry:imagestreamtags_count:sum
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: ingress-operator
spec:
  groups:
  - name: openshift-ingress.rules
    rules:
    - alert: HAProxyReloadFail
      annotations:
        description: This alert fires when HAProxy fails to reload its configuration,
          which will result in the router not picking up recently created or modified
          routes.
        message: HAProxy reloads are failing on {{ $labels.pod }}. Router is not respecting
          recently created or modified routes
        summary: HAProxy reload failure
      expr: template_router_reload_failure == 1
      for: 5m
      labels:
        severity: warning
    - alert: HAProxyDown
      annotations:
        description: This alert fires when metrics report that HAProxy is down.
        message: HAProxy metrics are reporting that HAProxy is down on pod {{ $labels.namespace
          }} / {{ $labels.pod }}
        summary: HAProxy is down
      expr: haproxy_up == 0
      for: 5m
      labels:
        severity: critical
    - alert: IngressControllerDegraded
      annotations:
        description: This alert fires when the IngressController status is degraded.
        message: |
          The {{ $labels.namespace }}/{{ $labels.name }} ingresscontroller is
          degraded: {{ $labels.reason }}.
        summary: IngressController is degraded
      expr: ingress_controller_conditions{condition="Degraded"} == 1
      for: 5m
      labels:
        severity: warning
    - alert: IngressControllerUnavailable
      annotations:
        description: This alert fires when the IngressController is not available.
        message: |
          The {{ $labels.namespace }}/{{ $labels.name }} ingresscontroller is
          unavailable: {{ $labels.reason }}.
        summary: IngressController is unavailable
      expr: ingress_controller_conditions{condition="Available"} == 0
      for: 5m
      labels:
        severity: warning
    - expr: min(route_metrics_controller_routes_per_shard)
      record: cluster:route_metrics_controller_routes_per_shard:min
    - expr: max(route_metrics_controller_routes_per_shard)
      record: cluster:route_metrics_controller_routes_per_shard:max
    - expr: avg(route_metrics_controller_routes_per_shard)
      record: cluster:route_metrics_controller_routes_per_shard:avg
    - expr: quantile(0.5, route_metrics_controller_routes_per_shard)
      record: cluster:route_metrics_controller_routes_per_shard:median
    - expr: sum (openshift_route_info) by (tls_termination)
      record: cluster:openshift_route_info:tls_termination:sum
  - name: openshift-ingress-to-route-controller.rules
    rules:
    - alert: IngressWithoutClassName
      annotations:
        description: This alert fires when there is an Ingress with an unset IngressClassName
          for longer than one day.
        message: Ingress {{ $labels.namespace }}/{{ $labels.name }} is missing the
          IngressClassName for 1 day.
        summary: Ingress without IngressClassName for 1 day
      expr: openshift_ingress_to_route_controller_ingress_without_class_name == 1
      for: 1d
      labels:
        severity: warning
    - alert: UnmanagedRoutes
      annotations:
        description: This alert fires when there is a Route owned by an unmanaged
          Ingress.
        message: Route {{ $labels.namespace }}/{{ $labels.name }} is owned by an unmanaged
          Ingress.
        summary: Route owned by an Ingress no longer managed
      expr: openshift_ingress_to_route_controller_route_with_unmanaged_owner == 1
      for: 1h
      labels:
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: insights-prometheus-rules
spec:
  groups:
  - name: insights
    rules:
    - alert: InsightsDisabled
      annotations:
        description: 'Insights operator is disabled. In order to enable Insights and
          benefit from recommendations specific to your cluster, please follow steps
          listed in the documentation: https://docs.openshift.com/container-platform/latest/support/remote_health_monitoring/enabling-remote-health-reporting.html'
        summary: Insights operator is disabled.
      expr: max without (job, pod, service, instance) (cluster_operator_conditions{name="insights",
        condition="Disabled"} == 1)
      for: 5m
      labels:
        namespace: openshift-insights
        severity: info
    - alert: SimpleContentAccessNotAvailable
      annotations:
        description: Simple content access (SCA) is not enabled. Once enabled, Insights
          Operator can automatically import the SCA certificates from Red Hat OpenShift
          Cluster Manager making it easier to use the content provided by your Red
          Hat subscriptions when creating container images. See https://docs.openshift.com/container-platform/latest/cicd/builds/running-entitled-builds.html
          for more information.
        summary: Simple content access certificates are not available.
      expr: ' max without (job, pod, service, instance) (max_over_time(cluster_operator_conditions{name="insights",
        condition="SCAAvailable", reason="NotFound"}[5m]) == 0)'
      for: 5m
      labels:
        namespace: openshift-insights
        severity: info
    - alert: InsightsRecommendationActive
      annotations:
        description: Insights recommendation "{{ $labels.description }}" with total
          risk "{{ $labels.total_risk }}" was detected on the cluster. More information
          is available at {{ $labels.info_link }}.
        summary: An Insights recommendation is active for this cluster.
      expr: insights_recommendation_active == 1
      for: 5m
      labels:
        severity: info
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: kubernetes-monitoring-rules
spec:
  groups:
  - name: kubernetes-apps
    rules:
    - alert: KubePodCrashLooping
      annotations:
        description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
          }}) is in waiting state (reason: "CrashLoopBackOff").'
        summary: Pod is crash looping.
      expr: |
        max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff", namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m]) >= 1
      for: 15m
      labels:
        severity: warning
    - alert: KubePodNotReady
      annotations:
        description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
          state for longer than 15 minutes.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePodNotReady.md
        summary: Pod has been in a non-ready state for more than 15 minutes.
      expr: |
        sum by (namespace, pod, cluster) (
          max by(namespace, pod, cluster) (
            kube_pod_status_phase{namespace=~"(openshift-.*|kube-.*|default)", job="kube-state-metrics", phase=~"Pending|Unknown"}
            unless ignoring(phase) (kube_pod_status_unschedulable{job="kube-state-metrics"} == 1)
          ) * on(namespace, pod, cluster) group_left(owner_kind) topk by(namespace, pod, cluster) (
            1, max by(namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!="Job"})
          )
        ) > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeDeploymentGenerationMismatch
      annotations:
        description: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment
          }} does not match, this indicates that the Deployment has failed but has
          not been rolled back.
        summary: Deployment generation mismatch due to possible roll-back
      expr: |
        kube_deployment_status_observed_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          !=
        kube_deployment_metadata_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
      for: 15m
      labels:
        severity: warning
    - alert: KubeDeploymentRolloutStuck
      annotations:
        description: Rollout of deployment {{ $labels.namespace }}/{{ $labels.deployment
          }} is not progressing for longer than 15 minutes.
        summary: Deployment rollout is not progressing.
      expr: |
        kube_deployment_status_condition{condition="Progressing", status="false",namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
        != 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetReplicasMismatch
      annotations:
        description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }}
          has not matched the expected number of replicas for longer than 15 minutes.
        summary: StatefulSet has not matched the expected number of replicas.
      expr: |
        (
          kube_statefulset_status_replicas_ready{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            !=
          kube_statefulset_status_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
        ) and (
          changes(kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[10m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetGenerationMismatch
      annotations:
        description: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
          }} does not match, this indicates that the StatefulSet has failed but has
          not been rolled back.
        summary: StatefulSet generation mismatch due to possible roll-back
      expr: |
        kube_statefulset_status_observed_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          !=
        kube_statefulset_metadata_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetUpdateNotRolledOut
      annotations:
        description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }}
          update has not been rolled out.
        summary: StatefulSet update has not been rolled out.
      expr: |
        (
          max without (revision) (
            kube_statefulset_status_current_revision{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              unless
            kube_statefulset_status_update_revision{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          )
            *
          (
            kube_statefulset_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              !=
            kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          )
        )  and (
          changes(kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeDaemonSetRolloutStuck
      annotations:
        description: DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has
          not finished or progressed for at least 30 minutes.
        summary: DaemonSet rollout is stuck.
      expr: |
        (
          (
            kube_daemonset_status_current_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
             !=
            kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          ) or (
            kube_daemonset_status_number_misscheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
             !=
            0
          ) or (
            kube_daemonset_status_updated_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
             !=
            kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          ) or (
            kube_daemonset_status_number_available{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
             !=
            kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          )
        ) and (
          changes(kube_daemonset_status_updated_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m])
            ==
          0
        )
      for: 30m
      labels:
        severity: warning
    - alert: KubeContainerWaiting
      annotations:
        description: pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on
          container {{ $labels.container}} has been in waiting state for longer than
          1 hour.
        summary: Pod container waiting longer than 1 hour
      expr: |
        sum by (namespace, pod, container, cluster) (kube_pod_container_status_waiting_reason{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}) > 0
      for: 1h
      labels:
        severity: warning
    - alert: KubeDaemonSetNotScheduled
      annotations:
        description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
          }} are not scheduled.'
        summary: DaemonSet pods are not scheduled.
      expr: |
        kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          -
        kube_daemonset_status_current_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0
      for: 10m
      labels:
        severity: warning
    - alert: KubeDaemonSetMisScheduled
      annotations:
        description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
          }} are running where they are not supposed to run.'
        summary: DaemonSet pods are misscheduled.
      expr: |
        kube_daemonset_status_number_misscheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeJobNotCompleted
      annotations:
        description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking
          more than {{ "43200" | humanizeDuration }} to complete.
        summary: Job did not complete in time
      expr: |
        time() - max by(namespace, job_name, cluster) (kube_job_status_start_time{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          and
        kube_job_status_active{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0) > 43200
      labels:
        severity: warning
    - alert: KubeJobFailed
      annotations:
        description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to
          complete. Removing failed job after investigation should clear this alert.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeJobFailed.md
        summary: Job failed to complete.
      expr: |
        kube_job_failed{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}  > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeHpaReplicasMismatch
      annotations:
        description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }}
          has not matched the desired number of replicas for longer than 15 minutes.
        summary: HPA has not matched desired number of replicas.
      expr: |
        (kube_horizontalpodautoscaler_status_desired_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          !=
        kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"})
          and
        (kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          >
        kube_horizontalpodautoscaler_spec_min_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"})
          and
        (kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          <
        kube_horizontalpodautoscaler_spec_max_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"})
          and
        changes(kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[15m]) == 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeHpaMaxedOut
      annotations:
        description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }}
          has been running at max replicas for longer than 15 minutes.
        summary: HPA is running at max replicas
      expr: |
        kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          ==
        kube_horizontalpodautoscaler_spec_max_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
      for: 15m
      labels:
        severity: warning
  - name: kubernetes-resources
    rules:
    - alert: KubeCPUOvercommit
      annotations:
        description: Cluster {{ $labels.cluster }} has overcommitted CPU resource
          requests for Pods by {{ $value }} CPU shares and cannot tolerate node failure.
        summary: Cluster has overcommitted CPU resource requests.
      expr: |
        sum(namespace_cpu:kube_pod_container_resource_requests:sum{job="kube-state-metrics",}) by (cluster) - (sum(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster) - max(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster)) > 0
        and
        (sum(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster) - max(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster)) > 0
      for: 10m
      labels:
        namespace: kube-system
        severity: warning
    - alert: KubeMemoryOvercommit
      annotations:
        description: Cluster {{ $labels.cluster }} has overcommitted memory resource
          requests for Pods by {{ $value | humanize }} bytes and cannot tolerate node
          failure.
        summary: Cluster has overcommitted memory resource requests.
      expr: |
        sum(namespace_memory:kube_pod_container_resource_requests:sum{}) by (cluster) - (sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster) - max(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)) > 0
        and
        (sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster) - max(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)) > 0
      for: 10m
      labels:
        namespace: kube-system
        severity: warning
    - alert: KubeQuotaAlmostFull
      annotations:
        description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
          }} of its {{ $labels.resource }} quota.
        summary: Namespace quota is going to be full.
      expr: |
        kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="used"}
          / ignoring(instance, job, type)
        (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard"} > 0)
          > 0.9 < 1
      for: 15m
      labels:
        severity: info
    - alert: KubeQuotaFullyUsed
      annotations:
        description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
          }} of its {{ $labels.resource }} quota.
        summary: Namespace quota is fully used.
      expr: |
        kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="used"}
          / ignoring(instance, job, type)
        (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard"} > 0)
          == 1
      for: 15m
      labels:
        severity: info
    - alert: KubeQuotaExceeded
      annotations:
        description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
          }} of its {{ $labels.resource }} quota.
        summary: Namespace quota has exceeded the limits.
      expr: |
        kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="used"}
          / ignoring(instance, job, type)
        (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard"} > 0)
          > 1
      for: 15m
      labels:
        severity: warning
  - name: kubernetes-system
    rules:
    - alert: KubeClientErrors
      annotations:
        description: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance
          }}' is experiencing {{ $value | humanizePercentage }} errors.'
        summary: Kubernetes API server client is experiencing errors.
      expr: |
        (sum(rate(rest_client_requests_total{job="apiserver",code=~"5.."}[5m])) by (cluster, instance, job, namespace)
          /
        sum(rate(rest_client_requests_total{job="apiserver"}[5m])) by (cluster, instance, job, namespace))
        > 0.01
      for: 15m
      labels:
        severity: warning
  - name: kubernetes-system-kubelet
    rules:
    - alert: KubeNodeNotReady
      annotations:
        description: '{{ $labels.node }} has been unready for more than 15 minutes.'
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeNodeNotReady.md
        summary: Node is not ready.
      expr: |
        kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeNodeUnreachable
      annotations:
        description: '{{ $labels.node }} is unreachable and some workloads may be
          rescheduled.'
        summary: Node is unreachable.
      expr: |
        (kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} unless ignoring(key,value) kube_node_spec_taint{job="kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"}) == 1
      for: 15m
      labels:
        severity: warning
    - alert: KubeletTooManyPods
      annotations:
        description: Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage
          }} of its Pod capacity.
        summary: Kubelet is running at capacity.
      expr: |
        count by(cluster, node) (
          (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info{job="kube-state-metrics"})
        )
        /
        max by(cluster, node) (
          kube_node_status_capacity{job="kube-state-metrics",resource="pods"} != 1
        ) > 0.95
      for: 15m
      labels:
        namespace: kube-system
        severity: info
    - alert: KubeNodeReadinessFlapping
      annotations:
        description: The readiness status of node {{ $labels.node }} has changed {{
          $value }} times in the last 15 minutes.
        summary: Node readiness status is flapping.
      expr: |
        sum(changes(kube_node_status_condition{job="kube-state-metrics",status="true",condition="Ready"}[15m])) by (cluster, node) > 2
      for: 15m
      labels:
        namespace: kube-system
        severity: warning
    - alert: KubeletPlegDurationHigh
      annotations:
        description: The Kubelet Pod Lifecycle Event Generator has a 99th percentile
          duration of {{ $value }} seconds on node {{ $labels.node }}.
        summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.
      expr: |
        node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"} >= 10
      for: 5m
      labels:
        namespace: kube-system
        severity: warning
    - alert: KubeletPodStartUpLatencyHigh
      annotations:
        description: Kubelet Pod startup 99th percentile latency is {{ $value }} seconds
          on node {{ $labels.node }}.
        summary: Kubelet Pod startup latency is too high.
      expr: |
        histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le)) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"} > 60
      for: 15m
      labels:
        namespace: kube-system
        severity: warning
    - alert: KubeletClientCertificateRenewalErrors
      annotations:
        description: Kubelet on node {{ $labels.node }} has failed to renew its client
          certificate ({{ $value | humanize }} errors in the last 5 minutes).
        summary: Kubelet has failed to renew its client certificate.
      expr: |
        increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeletServerCertificateRenewalErrors
      annotations:
        description: Kubelet on node {{ $labels.node }} has failed to renew its server
          certificate ({{ $value | humanize }} errors in the last 5 minutes).
        summary: Kubelet has failed to renew its server certificate.
      expr: |
        increase(kubelet_server_expiration_renew_errors[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeletDown
      annotations:
        description: Kubelet has disappeared from Prometheus target discovery.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeletDown.md
        summary: Target disappeared from Prometheus target discovery.
      expr: |
        absent(up{job="kubelet", metrics_path="/metrics"} == 1)
      for: 15m
      labels:
        namespace: kube-system
        severity: critical
  - name: k8s.rules.container_cpu_usage_seconds_total
    rules:
    - expr: |
        sum by (cluster, namespace, pod, container) (
          irate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}[5m])
        ) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (
          1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate
  - name: k8s.rules.container_memory_working_set_bytes
    rules:
    - expr: |
        container_memory_working_set_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
        * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
          max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_memory_working_set_bytes
  - name: k8s.rules.container_memory_rss
    rules:
    - expr: |
        container_memory_rss{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
        * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
          max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_memory_rss
  - name: k8s.rules.container_memory_cache
    rules:
    - expr: |
        container_memory_cache{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
        * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
          max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_memory_cache
  - name: k8s.rules.container_memory_swap
    rules:
    - expr: |
        container_memory_swap{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
        * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
          max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_memory_swap
  - name: k8s.rules.container_resource
    rules:
    - expr: |
        kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}  * on (namespace, pod, cluster)
        group_left() max by (namespace, pod, cluster) (
          (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
        )
      record: cluster:namespace:pod_memory:active:kube_pod_container_resource_requests
    - expr: |
        sum by (namespace, cluster) (
            sum by (namespace, pod, cluster) (
                max by (namespace, pod, container, cluster) (
                  kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}
                ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                  kube_pod_status_phase{phase=~"Pending|Running"} == 1
                )
            )
        )
      record: namespace_memory:kube_pod_container_resource_requests:sum
    - expr: |
        kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}  * on (namespace, pod, cluster)
        group_left() max by (namespace, pod, cluster) (
          (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
        )
      record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests
    - expr: |
        sum by (namespace, cluster) (
            sum by (namespace, pod, cluster) (
                max by (namespace, pod, container, cluster) (
                  kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}
                ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                  kube_pod_status_phase{phase=~"Pending|Running"} == 1
                )
            )
        )
      record: namespace_cpu:kube_pod_container_resource_requests:sum
    - expr: |
        kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}  * on (namespace, pod, cluster)
        group_left() max by (namespace, pod, cluster) (
          (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
        )
      record: cluster:namespace:pod_memory:active:kube_pod_container_resource_limits
    - expr: |
        sum by (namespace, cluster) (
            sum by (namespace, pod, cluster) (
                max by (namespace, pod, container, cluster) (
                  kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}
                ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                  kube_pod_status_phase{phase=~"Pending|Running"} == 1
                )
            )
        )
      record: namespace_memory:kube_pod_container_resource_limits:sum
    - expr: |
        kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}  * on (namespace, pod, cluster)
        group_left() max by (namespace, pod, cluster) (
         (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
         )
      record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits
    - expr: |
        sum by (namespace, cluster) (
            sum by (namespace, pod, cluster) (
                max by (namespace, pod, container, cluster) (
                  kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}
                ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                  kube_pod_status_phase{phase=~"Pending|Running"} == 1
                )
            )
        )
      record: namespace_cpu:kube_pod_container_resource_limits:sum
  - name: k8s.rules.pod_owner
    rules:
    - expr: |
        max by (cluster, namespace, workload, pod) (
          label_replace(
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
              "replicaset", "$1", "owner_name", "(.*)"
            ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (
              1, max by (replicaset, namespace, owner_name) (
                kube_replicaset_owner{job="kube-state-metrics"}
              )
            ),
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: deployment
      record: namespace_workload_pod:kube_pod_owner:relabel
    - expr: |
        max by (cluster, namespace, workload, pod) (
          label_replace(
            kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: daemonset
      record: namespace_workload_pod:kube_pod_owner:relabel
    - expr: |
        max by (cluster, namespace, workload, pod) (
          label_replace(
            kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: statefulset
      record: namespace_workload_pod:kube_pod_owner:relabel
    - expr: |
        max by (cluster, namespace, workload, pod) (
          label_replace(
            kube_pod_owner{job="kube-state-metrics", owner_kind="Job"},
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: job
      record: namespace_workload_pod:kube_pod_owner:relabel
  - name: kube-scheduler.rules
    rules:
    - expr: |
        histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.99"
      record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.99"
      record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.99"
      record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.9"
      record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.9"
      record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.9"
      record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.5"
      record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.5"
      record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.5"
      record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
  - name: node.rules
    rules:
    - expr: |
        topk by(cluster, namespace, pod) (1,
          max by (cluster, node, namespace, pod) (
            label_replace(kube_pod_info{job="kube-state-metrics",node!=""}, "pod", "$1", "pod", "(.*)")
        ))
      record: 'node_namespace_pod:kube_pod_info:'
    - expr: |
        sum(
          node_memory_MemAvailable_bytes{job="node-exporter"} or
          (
            node_memory_Buffers_bytes{job="node-exporter"} +
            node_memory_Cached_bytes{job="node-exporter"} +
            node_memory_MemFree_bytes{job="node-exporter"} +
            node_memory_Slab_bytes{job="node-exporter"}
          )
        ) by (cluster)
      record: :node_memory_MemAvailable_bytes:sum
    - expr: |
        avg by (cluster, node) (
          sum without (mode) (
            rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal",job="node-exporter"}[5m])
          )
        )
      record: node:node_cpu_utilization:ratio_rate5m
    - expr: |
        avg by (cluster) (
          node:node_cpu_utilization:ratio_rate5m
        )
      record: cluster:node_cpu:ratio_rate5m
  - name: kubelet.rules
    rules:
    - expr: |
        histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
      labels:
        quantile: "0.99"
      record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
      labels:
        quantile: "0.9"
      record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
      labels:
        quantile: "0.5"
      record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: kube-state-metrics-rules
spec:
  groups:
  - name: kube-state-metrics
    rules:
    - alert: KubeStateMetricsListErrors
      annotations:
        description: kube-state-metrics is experiencing errors at an elevated rate
          in list operations. This is likely causing it to not be able to expose metrics
          about Kubernetes objects correctly or at all.
        summary: kube-state-metrics is experiencing errors in list operations.
      expr: |
        (sum(rate(kube_state_metrics_list_total{job="kube-state-metrics",result="error"}[5m])) by (cluster)
          /
        sum(rate(kube_state_metrics_list_total{job="kube-state-metrics"}[5m])) by (cluster))
        > 0.01
      for: 15m
      labels:
        namespace: openshift-monitoring
        severity: warning
    - alert: KubeStateMetricsWatchErrors
      annotations:
        description: kube-state-metrics is experiencing errors at an elevated rate
          in watch operations. This is likely causing it to not be able to expose
          metrics about Kubernetes objects correctly or at all.
        summary: kube-state-metrics is experiencing errors in watch operations.
      expr: |
        (sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics",result="error"}[5m])) by (cluster)
          /
        sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics"}[5m])) by (cluster))
        > 0.01
      for: 15m
      labels:
        namespace: openshift-monitoring
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: machine-config-controller
spec:
  groups:
  - name: os-image-override.rules
    rules:
    - expr: sum(os_image_url_override)
      record: os_image_url_override:sum
  - name: mcc-drain-error
    rules:
    - alert: MCCDrainError
      annotations:
        description: 'Drain failed on {{ $labels.exported_node }} , updates may be
          blocked. For more details check MachineConfigController pod logs: oc logs
          -f -n {{ $labels.namespace }} machine-config-controller-xxxxx -c machine-config-controller'
        summary: Alerts the user to a failed node drain. Always triggers when the
          failure happens one or more times.
      expr: |
        mcc_drain_err > 0
      labels:
        namespace: openshift-machine-config-operator
        severity: warning
  - name: mcc-pool-alert
    rules:
    - alert: MCCPoolAlert
      annotations:
        description: 'Node {{ $labels.exported_node }} has triggered a pool alert
          due to a label change. For more details check MachineConfigController pod
          logs: oc logs -f -n {{ $labels.namespace }} machine-config-controller-xxxxx
          -c machine-config-controller'
        summary: Triggers when nodes in a pool have overlapping labels such as master,
          worker, and a custom label therefore a choice must be made as to which is
          honored.
      expr: |
        mcc_pool_alert > 0
      labels:
        namespace: openshift-machine-config-operator
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: machine-config-daemon
spec:
  groups:
  - name: mcd-reboot-error
    rules:
    - alert: MCDRebootError
      annotations:
        description: 'Reboot failed on {{ $labels.node }} , update may be blocked.
          For more details:  oc logs -f -n {{ $labels.namespace }} {{ $labels.pod
          }} -c machine-config-daemon '
        summary: Alerts the user that a node failed to reboot one or more times over
          a span of 5 minutes.
      expr: |
        mcd_reboots_failed_total > 0
      for: 5m
      labels:
        namespace: openshift-machine-config-operator
        severity: critical
  - name: mcd-pivot-error
    rules:
    - alert: MCDPivotError
      annotations:
        description: 'Error detected in pivot logs on {{ $labels.node }} , upgrade
          may be blocked. For more details:  oc logs -f -n {{ $labels.namespace }}
          {{ $labels.pod }} -c machine-config-daemon '
        summary: Alerts the user when an error is detected upon pivot. This triggers
          if the pivot errors are above zero for 2 minutes.
      expr: |
        mcd_pivot_errors_total > 0
      for: 2m
      labels:
        namespace: openshift-machine-config-operator
        severity: warning
  - name: mcd-kubelet-health-state-error
    rules:
    - alert: KubeletHealthState
      annotations:
        description: Kubelet health failure threshold reached
        summary: This keeps track of Kubelet health failures, and tallys them. The
          warning is triggered if 2 or more failures occur.
      expr: |
        mcd_kubelet_state > 2
      labels:
        namespace: openshift-machine-config-operator
        severity: warning
  - name: system-memory-exceeds-reservation
    rules:
    - alert: SystemMemoryExceedsReservation
      annotations:
        description: System memory usage of {{ $value | humanize }} on {{ $labels.node
          }} exceeds 95% of the reservation. Reserved memory ensures system processes
          can function even when the node is fully allocated and protects against
          workload out of memory events impacting the proper functioning of the node.
          The default reservation is expected to be sufficient for most configurations
          and should be increased (https://docs.openshift.com/container-platform/latest/nodes/nodes/nodes-nodes-managing.html)
          when running nodes with high numbers of pods (either due to rate of change
          or at steady state).
        summary: Alerts the user when, for 15 miutes, a specific node is using more
          memory than is reserved
      expr: |
        sum by (node) (container_memory_rss{id="/system.slice"}) > ((sum by (node) (kube_node_status_capacity{resource="memory"} - kube_node_status_allocatable{resource="memory"})) * 0.95)
      for: 15m
      labels:
        namespace: openshift-machine-config-operator
        severity: warning
  - name: high-overall-control-plane-memory
    rules:
    - alert: HighOverallControlPlaneMemory
      annotations:
        description: Given three control plane nodes, the overall memory utilization
          may only be about 2/3 of all available capacity. This is because if a single
          control plane node fails, the kube-apiserver and etcd may be slow to respond.
          To fix this, increase memory of the control plane nodes.
        summary: Memory utilization across all control plane nodes is high, and could
          impact responsiveness and stability.
      expr: |
        (
          1
          -
          sum (
            node_memory_MemFree_bytes
            + node_memory_Buffers_bytes
            + node_memory_Cached_bytes
            AND on (instance)
            label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )
          ) / sum (
            node_memory_MemTotal_bytes
            AND on (instance)
            label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )
          )
        ) * 100 > 60
      for: 1h
      labels:
        namespace: openshift-machine-config-operator
        severity: warning
  - name: extremely-high-individual-control-plane-memory
    rules:
    - alert: ExtremelyHighIndividualControlPlaneMemory
      annotations:
        description: The memory utilization per instance within control plane nodes
          influence the stability, and responsiveness of the cluster. This can lead
          to cluster instability and slow responses from kube-apiserver or failing
          requests specially on etcd. Moreover, OOM kill is expected which negatively
          influences the pod scheduling. If this happens on container level, the descheduler
          will not be able to detect it, as it works on the pod level. To fix this,
          increase memory of the affected node of control plane nodes.
        summary: Extreme memory utilization per node within control plane nodes is
          extremely high, and could impact responsiveness and stability.
      expr: |
        (
          1
          -
          sum by (instance) (
            node_memory_MemFree_bytes
            + node_memory_Buffers_bytes
            + node_memory_Cached_bytes
            AND on (instance)
            label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )
          ) / sum by (instance) (
            node_memory_MemTotal_bytes
            AND on (instance)
            label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )
          )
        ) * 100 > 90
      for: 45m
      labels:
        namespace: openshift-machine-config-operator
        severity: critical
  - name: mcd-missing-mc
    rules:
    - alert: MissingMachineConfig
      annotations:
        description: Could not find config {{ $labels.mc }} in-cluster, this likely
          indicates the MachineConfigs in-cluster has changed during the install process.  If
          you are seeing this when installing the cluster, please compare the in-cluster
          rendered machineconfigs to /etc/mcs-machine-config-content.json
        summary: This keeps track of Machine Config failures. Specifically a common
          failure on install when a rendered Machine Config is missing. Triggered
          when this error happens once.
      expr: |
        mcd_missing_mc > 0
      labels:
        namespace: openshift-machine-config-operator
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: machine-config-operator
spec:
  groups:
  - name: drain-override-configmap-present
    rules:
    - alert: MCODrainOverrideConfigMapAlert
      annotations:
        description: Image Registry Drain Override configmap has been detected. Please
          use the Node Disruption Policy feature to control the cluster's drain behavior
          as the configmap method is currently deprecated and will be removed in a
          future release.
        summary: Alerts the user to the presence of a drain override configmap that
          is being deprecated and removed in a future release.
      expr: |
        mco_image_registry_drain_override_exists > 0
      labels:
        namespace: openshift-machine-config-operator
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: marketplace-alert-rules
spec:
  groups:
  - name: operator.marketplace.rules
    rules:
    - alert: OperatorHubSourceError
      annotations:
        description: Operators shipped via the {{ $labels.name }} source are not available
          for installation until the issue is fixed. Operators already installed from
          this source will not receive updates until issue is fixed. Inspect the status
          of the pod owned by {{ $labels.name }} source in the openshift-marketplace
          namespace (oc -n openshift-marketplace get pods -l olm.catalogSource={{
          $labels.name }}) to diagnose and repair.
        summary: The {{ $labels.name }} source is in non-ready state for more than
          10 minutes.
      expr: catalogsource_ready{exported_namespace="openshift-marketplace"} == 0
      for: 10m
      labels:
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: networking-rules
spec:
  groups:
  - name: cluster-network-operator-ovn.rules
    rules:
    - alert: NodeWithoutOVNKubeNodePodRunning
      annotations:
        description: |
          Networking is degraded on nodes that do not have a functioning ovnkube-node pod. Existing workloads on the
          node may continue to have connectivity but any changes to the networking control plane will not be implemented.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NodeWithoutOVNKubeNodePodRunning.md
        summary: All Linux nodes should be running an ovnkube-node pod, {{ $labels.node
          }} is not.
      expr: |
        (kube_node_info unless on(node) (kube_pod_info{namespace="openshift-ovn-kubernetes",pod=~"ovnkube-node.*"}
        or kube_node_labels{label_kubernetes_io_os="windows"})) > 0
      for: 20m
      labels:
        severity: warning
    - alert: OVNKubernetesControllerDisconnectedSouthboundDatabase
      annotations:
        description: |
          Networking is degraded on nodes when OVN controller is not connected to OVN southbound database connection. No networking control plane updates will be applied to the node.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/OVNKubernetesControllerDisconnectedSouthboundDatabase.md
        summary: Networking control plane is degraded on node {{ $labels.node }} because
          OVN controller is not connected to OVN southbound database.
      expr: |
        max_over_time(ovn_controller_southbound_database_connected[5m]) == 0
      for: 10m
      labels:
        severity: warning
    - alert: OVNKubernetesNodePodAddError
      annotations:
        description: OVN Kubernetes experiences pod creation errors at an elevated
          rate. The pods will be retried.
        summary: OVN Kubernetes is experiencing pod creation errors at an elevated
          rate.
      expr: |
        (sum by(instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command="ADD",err="true"}[5m]))
          /
        sum by(instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command="ADD"}[5m])))
        > 0.1
      for: 15m
      labels:
        severity: warning
    - alert: OVNKubernetesNodePodDeleteError
      annotations:
        description: OVN Kubernetes experiences pod deletion errors at an elevated
          rate. The pods will be retried.
        summary: OVN Kubernetes experiencing pod deletion errors at an elevated rate.
      expr: |
        (sum by(instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command="DEL",err="true"}[5m]))
          /
        sum by(instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command="DEL"}[5m])))
        > 0.1
      for: 15m
      labels:
        severity: warning
    - alert: OVNKubernetesResourceRetryFailure
      annotations:
        description: |
          OVN Kubernetes failed to apply networking control plane configuration after several attempts. This might be because the configuration
          provided by the user is invalid or because of an internal error. As a consequence, the cluster might have a degraded status.
        summary: OVN Kubernetes failed to apply networking control plane configuration.
      expr: increase(ovnkube_resource_retry_failures_total[10m]) > 0
      labels:
        severity: warning
    - alert: OVNKubernetesNodeOVSOverflowUserspace
      annotations:
        description: Netlink messages dropped by OVS vSwitch daemon due to netlink
          socket buffer overflow. This will result in packet loss.
        summary: OVS vSwitch daemon drops packets due to buffer overflow.
      expr: increase(ovs_vswitchd_netlink_overflow[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: OVNKubernetesNodeOVSOverflowKernel
      annotations:
        description: Netlink messages dropped by OVS kernel module due to netlink
          socket buffer overflow. This will result in packet loss.
        summary: OVS kernel module drops packets due to buffer overflow.
      expr: increase(ovs_vswitchd_dp_flows_lookup_lost[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: NorthboundStale
      annotations:
        description: |
          OVN-Kubernetes controller and/or OVN northbound database may cause a
          degraded networking control plane for the affected node. Existing
          workloads should continue to have connectivity but new workloads may
          be impacted.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NorthboundStaleAlert.md
        summary: OVN-Kubernetes controller {{ $labels.instance }} has not successfully
          synced any changes to the northbound database for too long.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        time() - max_over_time(ovnkube_controller_nb_e2e_timestamp[5m]) > 120
      for: 10m
      labels:
        severity: warning
    - alert: SouthboundStale
      annotations:
        description: |
          OVN-Kubernetes controller and/or OVN northbound database may cause a
          degraded networking control plane for the affected node. Existing
          workloads should continue to have connectivity but new workloads may
          be impacted.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/SouthboundStaleAlert.md
        summary: OVN northd {{ $labels.instance }} has not successfully synced any
          changes to the southbound database for too long.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(ovnkube_controller_nb_e2e_timestamp[5m]) - max_over_time(ovnkube_controller_sb_e2e_timestamp[5m]) > 120
      for: 10m
      labels:
        severity: warning
    - alert: OVNKubernetesNorthboundDatabaseCPUUsageHigh
      annotations:
        description: |
          High OVN northbound CPU usage indicates high load on the networking
          control plane for the affected node.
        summary: OVN northbound database {{ $labels.instance }} is greater than {{
          $value | humanizePercentage }} percent CPU usage for a period of time.
      expr: (sum(rate(container_cpu_usage_seconds_total{container="nbdb"}[5m])) BY
        (instance, name, namespace)) > 0.8
      for: 15m
      labels:
        severity: info
    - alert: OVNKubernetesSouthboundDatabaseCPUUsageHigh
      annotations:
        description: |
          High OVN southbound CPU usage indicates high load on the networking
          control plane for the affected node.
        summary: OVN southbound database {{ $labels.instance }} is greater than {{
          $value | humanizePercentage }} percent CPU usage for a period of time.
      expr: (sum(rate(container_cpu_usage_seconds_total{container="sbdb"}[5m])) BY
        (instance, name, namespace)) > 0.8
      for: 15m
      labels:
        severity: info
    - alert: OVNKubernetesNorthdInactive
      annotations:
        description: |
          An inactive OVN northd instance may cause a degraded networking
          control plane for the affected node. Existing workloads should
          continue to have connectivity but new workloads may be impacted.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/OVNKubernetesNorthdInactive.md
        summary: OVN northd {{ $labels.instance }} is not active.
      expr: count(ovn_northd_status != 1) BY (instance, name, namespace) > 0
      for: 10m
      labels:
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: node-exporter-rules
spec:
  groups:
  - name: node-exporter
    rules:
    - alert: NodeFilesystemSpaceFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
          }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
          space left and is filling up.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md
        summary: Filesystem is predicted to run out of space within the next 24 hours.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 15
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeFilesystemSpaceFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
          }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
          space left and is filling up fast.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md
        summary: Filesystem is predicted to run out of space within the next 4 hours.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 10
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 4*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
        )
      for: 1h
      labels:
        severity: critical
    - alert: NodeFilesystemAlmostOutOfSpace
      annotations:
        description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
          }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
          space left.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfSpace.md
        summary: Filesystem has less than 5% space left.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 5
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
        )
      for: 30m
      labels:
        severity: warning
    - alert: NodeFilesystemAlmostOutOfSpace
      annotations:
        description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
          }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
          space left.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfSpace.md
        summary: Filesystem has less than 3% space left.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 3
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
        )
      for: 30m
      labels:
        severity: critical
    - alert: NodeFilesystemFilesFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
          }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
          inodes left and is filling up.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md
        summary: Filesystem is predicted to run out of inodes within the next 24 hours.
      expr: |
        (
          node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 40
        and
          predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeFilesystemFilesFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
          }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
          inodes left and is filling up fast.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md
        summary: Filesystem is predicted to run out of inodes within the next 4 hours.
      expr: |
        (
          node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 20
        and
          predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 4*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
        )
      for: 1h
      labels:
        severity: critical
    - alert: NodeFilesystemAlmostOutOfFiles
      annotations:
        description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
          }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
          inodes left.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfFiles.md
        summary: Filesystem has less than 5% inodes left.
      expr: |
        (
          node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 5
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeFilesystemAlmostOutOfFiles
      annotations:
        description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
          }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
          inodes left.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfFiles.md
        summary: Filesystem has less than 3% inodes left.
      expr: |
        (
          node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 3
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
        )
      for: 1h
      labels:
        severity: critical
    - alert: NodeNetworkReceiveErrs
      annotations:
        description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
          {{ printf "%.0f" $value }} receive errors in the last two minutes.'
        summary: Network interface is reporting many receive errors.
      expr: |
        rate(node_network_receive_errs_total{job="node-exporter"}[2m]) / rate(node_network_receive_packets_total{job="node-exporter"}[2m]) > 0.01
      for: 1h
      labels:
        severity: warning
    - alert: NodeNetworkTransmitErrs
      annotations:
        description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
          {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
        summary: Network interface is reporting many transmit errors.
      expr: |
        rate(node_network_transmit_errs_total{job="node-exporter"}[2m]) / rate(node_network_transmit_packets_total{job="node-exporter"}[2m]) > 0.01
      for: 1h
      labels:
        severity: warning
    - alert: NodeHighNumberConntrackEntriesUsed
      annotations:
        description: '{{ $value | humanizePercentage }} of conntrack entries are used.'
        summary: Number of conntrack are getting close to the limit.
      expr: |
        (node_nf_conntrack_entries{job="node-exporter"} / node_nf_conntrack_entries_limit) > 0.75
      labels:
        severity: warning
    - alert: NodeTextFileCollectorScrapeError
      annotations:
        description: Node Exporter text file collector on {{ $labels.instance }} failed
          to scrape.
        summary: Node Exporter text file collector failed to scrape.
      expr: |
        node_textfile_scrape_error{job="node-exporter"} == 1
      labels:
        severity: warning
    - alert: NodeClockSkewDetected
      annotations:
        description: Clock at {{ $labels.instance }} is out of sync by more than 0.05s.
          Ensure NTP is configured correctly on this host.
        summary: Clock skew detected.
      expr: |-
        (
        (
          node_timex_offset_seconds{job="node-exporter"} > 0.05
        and
          deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) >= 0
        )
        or
        (
          node_timex_offset_seconds{job="node-exporter"} < -0.05
        and
          deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) <= 0
        )
        ) and on() absent(up{job="ptp-monitor-service"})
      for: 10m
      labels:
        severity: warning
    - alert: NodeClockNotSynchronising
      annotations:
        description: Clock at {{ $labels.instance }} is not synchronising. Ensure
          NTP is configured on this host.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeClockNotSynchronising.md
        summary: Clock not synchronising.
      expr: |-
        (
        min_over_time(node_timex_sync_status{job="node-exporter"}[5m]) == 0
        and
        node_timex_maxerror_seconds{job="node-exporter"} >= 16
        ) and on() absent(up{job="ptp-monitor-service"})
      for: 10m
      labels:
        severity: critical
    - alert: NodeRAIDDegraded
      annotations:
        description: RAID array '{{ $labels.device }}' at {{ $labels.instance }} is
          in degraded state due to one or more disks failures. Number of spare drives
          is insufficient to fix issue automatically.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeRAIDDegraded.md
        summary: RAID Array is degraded.
      expr: |
        node_md_disks_required{job="node-exporter",device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"} - ignoring (state) (node_md_disks{state="active",job="node-exporter",device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}) > 0
      for: 15m
      labels:
        severity: critical
    - alert: NodeRAIDDiskFailure
      annotations:
        description: At least one device in RAID array at {{ $labels.instance }} failed.
          Array '{{ $labels.device }}' needs attention and possibly a disk swap.
        summary: Failed device in RAID array.
      expr: |
        node_md_disks{state="failed",job="node-exporter",device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"} > 0
      labels:
        severity: warning
    - alert: NodeFileDescriptorLimit
      annotations:
        description: File descriptors limit at {{ $labels.instance }} is currently
          at {{ printf "%.2f" $value }}%.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFileDescriptorLimit.md
        summary: Kernel is predicted to exhaust file descriptors limit soon.
      expr: |
        (
          node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 70
        )
      for: 15m
      labels:
        severity: warning
    - alert: NodeFileDescriptorLimit
      annotations:
        description: File descriptors limit at {{ $labels.instance }} is currently
          at {{ printf "%.2f" $value }}%.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFileDescriptorLimit.md
        summary: Kernel is predicted to exhaust file descriptors limit soon.
      expr: |
        (
          node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 90
        )
      for: 15m
      labels:
        severity: critical
    - alert: NodeSystemSaturation
      annotations:
        description: |
          System load per core at {{ $labels.instance }} has been above 2 for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
          This might indicate this instance resources saturation and can cause it becoming unresponsive.
        summary: System saturated, load per core is very high.
      expr: |
        node_load1{job="node-exporter"}
        / count without (cpu, mode) (node_cpu_seconds_total{job="node-exporter", mode="idle"}) > 2
      for: 15m
      labels:
        severity: warning
    - alert: NodeMemoryMajorPagesFaults
      annotations:
        description: |
          Memory major pages are occurring at very high rate at {{ $labels.instance }}, 500 major page faults per second for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
          Please check that there is enough memory available at this instance.
        summary: Memory major page faults are occurring at very high rate.
      expr: |
        rate(node_vmstat_pgmajfault{job="node-exporter"}[5m]) > 500
      for: 15m
      labels:
        severity: warning
    - alert: NodeSystemdServiceFailed
      annotations:
        description: Systemd service {{ $labels.name }} has entered failed state at
          {{ $labels.instance }}
        summary: Systemd service has entered failed state.
      expr: |
        node_systemd_unit_state{job="node-exporter", state="failed"} == 1
      for: 15m
      labels:
        severity: warning
    - alert: NodeBondingDegraded
      annotations:
        description: Bonding interface {{ $labels.master }} on {{ $labels.instance
          }} is in degraded state due to one or more slave failures.
        summary: Bonding interface is degraded
      expr: |
        (node_bonding_slaves - node_bonding_active) != 0
      for: 5m
      labels:
        severity: warning
  - name: node-exporter.rules
    rules:
    - expr: |
        count without (cpu, mode) (
          node_cpu_seconds_total{job="node-exporter",mode="idle"}
        )
      record: instance:node_num_cpu:sum
    - expr: |
        1 - avg without (cpu) (
          sum without (mode) (rate(node_cpu_seconds_total{job="node-exporter", mode=~"idle|iowait|steal"}[1m]))
        )
      record: instance:node_cpu_utilisation:rate1m
    - expr: |
        (
          node_load1{job="node-exporter"}
        /
          instance:node_num_cpu:sum{job="node-exporter"}
        )
      record: instance:node_load1_per_cpu:ratio
    - expr: |
        1 - (
          (
            node_memory_MemAvailable_bytes{job="node-exporter"}
            or
            (
              node_memory_Buffers_bytes{job="node-exporter"}
              +
              node_memory_Cached_bytes{job="node-exporter"}
              +
              node_memory_MemFree_bytes{job="node-exporter"}
              +
              node_memory_Slab_bytes{job="node-exporter"}
            )
          )
        /
          node_memory_MemTotal_bytes{job="node-exporter"}
        )
      record: instance:node_memory_utilisation:ratio
    - expr: |
        rate(node_vmstat_pgmajfault{job="node-exporter"}[1m])
      record: instance:node_vmstat_pgmajfault:rate1m
    - expr: |
        rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
      record: instance_device:node_disk_io_time_seconds:rate1m
    - expr: |
        rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
      record: instance_device:node_disk_io_time_weighted_seconds:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_receive_bytes_excluding_lo:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_transmit_bytes_excluding_lo:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_receive_drop_excluding_lo:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_transmit_drop_excluding_lo:rate1m
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: node-tuning-operator
spec:
  groups:
  - name: node-tuning-operator.rules
    rules:
    - alert: NTOPodsNotReady
      annotations:
        description: |
          Pod {{ $labels.pod }} is not ready.
          Review the "Event" objects in "openshift-cluster-node-tuning-operator" namespace for further details.
        summary: Pod {{ $labels.pod }} is not ready.
      expr: |
        kube_pod_status_ready{namespace='openshift-cluster-node-tuning-operator', condition='true'} == 0
      for: 30m
      labels:
        severity: warning
    - alert: NTODegraded
      annotations:
        description: The Node Tuning Operator is degraded. Review the "node-tuning"
          ClusterOperator object for further details.
        summary: The Node Tuning Operator is degraded.
      expr: nto_degraded_info == 1
      for: 2h
      labels:
        severity: warning
    - expr: count by (_id) (nto_profile_calculated_total{profile!~"openshift-node",profile!~"openshift-control-plane",profile!~"openshift"})
      record: nto_custom_profiles:count
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: observability-operator
spec:
  groups:
  - name: operator
    rules:
    - alert: ClusterObservabilityOperatorReconciliationsFailed
      annotations:
        description: |-
          {{$value | humanize}}% of reconciliation requests are failing for the '{{ $labels.controller}}' controller.

          Check the logs of the {{$labels.namespace}}/{{$labels.pod}} pod to investigate further.
        summary: Cluster observability operator fails to reconcile resources
      expr: |-
        sum by(controller,pod,namespace) (rate(controller_runtime_reconcile_total{result="error",job="observability-operator"}[5m]))
        /
        sum by(controller,pod,namespace) (rate(controller_runtime_reconcile_total{job="observability-operator"}[5m])) > 0.1
      for: 15m
      labels:
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: olm-alert-rules
spec:
  groups:
  - name: olm.csv_abnormal.rules
    rules:
    - alert: CsvAbnormalFailedOver2Min
      annotations:
        description: Failed to install Operator {{ $labels.name }} version {{ $labels.version
          }}. Reason-{{ $labels.reason }}
        summary: CSV failed for over 2 minutes
      expr: last_over_time(csv_abnormal{phase="Failed"}[5m])
      for: 2m
      labels:
        severity: warning
    - alert: CsvAbnormalOver30Min
      annotations:
        description: Failed to install Operator {{ $labels.name }} version {{ $labels.version
          }}. Phase-{{ $labels.phase }} Reason-{{ $labels.reason }}
        summary: CSV abnormal for over 30 minutes
      expr: last_over_time(csv_abnormal{phase=~"(Replacing|Pending|Deleting|Unknown)"}[5m])
      for: 30m
      labels:
        severity: warning
  - name: olm.installplan.rules
    rules:
    - alert: InstallPlanStepAppliedWithWarnings
      annotations:
        description: The API server returned a warning during installation or upgrade
          of an operator. An Event with reason "AppliedWithWarnings" has been created
          with complete details, including a reference to the InstallPlan step that
          generated the warning.
        summary: API returned a warning when modifying an operator
      expr: sum by(namespace) (increase(installplan_warnings_total[5m])) > 0
      labels:
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: openshift-network-operator-ipsec-rules
spec:
  groups:
  - name: openshift-network.rules
    rules:
    - expr: |-
        group by (mode,is_legacy_api) (
          openshift_network_operator_ipsec_state{namespace=~"openshift-network-operator"}
        )
      record: openshift:openshift_network_operator_ipsec_state:info
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: podsecurity
spec:
  groups:
  - name: pod-security-violation
    rules:
    - alert: PodSecurityViolation
      annotations:
        description: A workload (pod, deployment, daemonset, ...) was created somewhere
          in the cluster but it did not match the PodSecurity "{{ $labels.policy_level
          }}" profile defined by its namespace either via the cluster-wide configuration
          (which triggers on a "restricted" profile violations) or by the namespace
          local Pod Security labels. Refer to Kubernetes documentation on Pod Security
          Admission to learn more about these violations.
        summary: One or more workloads users created in the cluster don't match their
          Pod Security profile
      expr: |
        sum(increase(pod_security_evaluations_total{decision="deny",mode="audit",resource="pod",ocp_namespace=""}[1d])) by (policy_level, ocp_namespace) > 0
      labels:
        namespace: openshift-kube-apiserver
        severity: info
    - alert: PodSecurityViolation
      annotations:
        description: A workload (pod, deployment, daemonset, ...) was created in namespace
          "{{ $labels.ocp_namespace }}" but it did not match the PodSecurity "{{ $labels.policy_level
          }}" profile defined by its namespace either via the cluster-wide configuration
          (which triggers on a "restricted" profile violations) or by the namespace
          local Pod Security labels. Refer to Kubernetes documentation on Pod Security
          Admission to learn more about these violations.
        summary: One or more workloads in platform namespaces of the cluster don't
          match their Pod Security profile
      expr: |
        sum(increase(pod_security_evaluations_total{decision="deny",mode="audit",resource="pod",ocp_namespace!=""}[1d])) by (policy_level, ocp_namespace) > 0
      labels:
        namespace: openshift-kube-apiserver
        severity: info
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: prometheus
spec:
  groups:
  - name: default-storage-classes.rules
    rules:
    - alert: MultipleDefaultStorageClasses
      annotations:
        description: |
          Cluster storage operator monitors all storage classes configured in the cluster
          and checks there is not more than one default StorageClass configured.
        message: StorageClass count check is failing (there should not be more than
          one default StorageClass)
        summary: More than one default StorageClass detected.
      expr: min_over_time(default_storage_class_count[5m]) > 1
      for: 10m
      labels:
        severity: warning
  - name: storage-operations.rules
    rules:
    - alert: PodStartupStorageOperationsFailing
      annotations:
        description: |
          Failing storage operation "{{ $labels.operation_name }}" of volume plugin {{ $labels.volume_plugin }} was preventing Pods on node {{ $labels.node }}
          from starting for past 5 minutes.
          Please investigate Pods that are "ContainerCreating" on the node: "oc get pod --field-selector=spec.nodeName={{ $labels.node }} --all-namespaces | grep ContainerCreating".
          Events of the Pods should contain exact error message: "oc describe pod -n <pod namespace> <pod name>".
        summary: Pods can't start because {{ $labels.operation_name }} of volume plugin
          {{ $labels.volume_plugin }} is permanently failing on node {{ $labels.node
          }}.
      expr: |
        increase(storage_operation_duration_seconds_count{status != "success", operation_name =~"volume_attach|volume_mount"}[5m]) > 0
          and ignoring(status) (sum without(status)
            (increase(storage_operation_duration_seconds_count{status = "success", operation_name =~"volume_attach|volume_mount"}[5m])
              or increase(storage_operation_duration_seconds_count{status != "success", operation_name =~"volume_attach|volume_mount"}[5m]) * 0)
            ) == 0
      for: 5m
      labels:
        severity: info
  - name: storage-selinux.rules
    rules:
    - expr: sum(volume_manager_selinux_pod_context_mismatch_warnings_total) + sum(volume_manager_selinux_pod_context_mismatch_errors_total)
      record: cluster:volume_manager_selinux_pod_context_mismatch_total
    - expr: sum by(volume_plugin) (volume_manager_selinux_volume_context_mismatch_warnings_total{volume_plugin
        !~".*-e2e-.*"})
      record: cluster:volume_manager_selinux_volume_context_mismatch_warnings_total
    - expr: sum by(volume_plugin) (volume_manager_selinux_volume_context_mismatch_errors_total{volume_plugin
        !~".*-e2e-.*"})
      record: cluster:volume_manager_selinux_volume_context_mismatch_errors_total
    - expr: sum by(volume_plugin) (volume_manager_selinux_volumes_admitted_total{volume_plugin
        !~".*-e2e-.*"})
      record: cluster:volume_manager_selinux_volumes_admitted_total
  - name: kubernetes-storage
    rules:
    - alert: KubePersistentVolumeFillingUp
      annotations:
        description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
          }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster
          {{ . }} {{- end }} is only {{ $value | humanizePercentage }} free.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeFillingUp.md
        summary: PersistentVolume is filling up.
      expr: |
        (
          kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
            /
          kubelet_volume_stats_capacity_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
        ) < 0.03
        and
        kubelet_volume_stats_used_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"} > 0
        unless on(cluster, namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_access_mode{namespace=~"(openshift-.*|kube-.*|default)", access_mode="ReadOnlyMany"} == 1
        unless on(cluster, namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_labels{namespace=~"(openshift-.*|kube-.*|default)",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
      for: 1m
      labels:
        severity: critical
    - alert: KubePersistentVolumeFillingUp
      annotations:
        description: Based on recent sampling, the PersistentVolume claimed by {{
          $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{
          with $labels.cluster -}} on Cluster {{ . }} {{- end }} is expected to fill
          up within four days. Currently {{ $value | humanizePercentage }} is available.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeFillingUp.md
        summary: PersistentVolume is filling up.
      expr: |
        (
          kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
            /
          kubelet_volume_stats_capacity_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
        ) < 0.15
        and
        kubelet_volume_stats_used_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"} > 0
        and
        predict_linear(kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
        unless on(cluster, namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_access_mode{namespace=~"(openshift-.*|kube-.*|default)", access_mode="ReadOnlyMany"} == 1
        unless on(cluster, namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_labels{namespace=~"(openshift-.*|kube-.*|default)",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
      for: 1h
      labels:
        severity: warning
    - alert: KubePersistentVolumeInodesFillingUp
      annotations:
        description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
          }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster
          {{ . }} {{- end }} only has {{ $value | humanizePercentage }} free inodes.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeInodesFillingUp.md
        summary: PersistentVolumeInodes are filling up.
      expr: |
        (
          kubelet_volume_stats_inodes_free{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
            /
          kubelet_volume_stats_inodes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
        ) < 0.03
        and
        kubelet_volume_stats_inodes_used{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"} > 0
        unless on(cluster, namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_access_mode{namespace=~"(openshift-.*|kube-.*|default)", access_mode="ReadOnlyMany"} == 1
        unless on(cluster, namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_labels{namespace=~"(openshift-.*|kube-.*|default)",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
      for: 1m
      labels:
        severity: critical
    - alert: KubePersistentVolumeInodesFillingUp
      annotations:
        description: Based on recent sampling, the PersistentVolume claimed by {{
          $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{
          with $labels.cluster -}} on Cluster {{ . }} {{- end }} is expected to run
          out of inodes within four days. Currently {{ $value | humanizePercentage
          }} of its inodes are free.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeInodesFillingUp.md
        summary: PersistentVolumeInodes are filling up.
      expr: |
        (
          kubelet_volume_stats_inodes_free{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
            /
          kubelet_volume_stats_inodes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
        ) < 0.15
        and
        kubelet_volume_stats_inodes_used{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"} > 0
        and
        predict_linear(kubelet_volume_stats_inodes_free{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
        unless on(cluster, namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_access_mode{namespace=~"(openshift-.*|kube-.*|default)", access_mode="ReadOnlyMany"} == 1
        unless on(cluster, namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_labels{namespace=~"(openshift-.*|kube-.*|default)",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
      for: 1h
      labels:
        severity: warning
    - alert: KubePersistentVolumeErrors
      annotations:
        description: The persistent volume {{ $labels.persistentvolume }} {{ with
          $labels.cluster -}} on Cluster {{ . }} {{- end }} has status {{ $labels.phase
          }}.
        summary: PersistentVolume is having issues with provisioning.
      expr: |
        kube_persistentvolume_status_phase{phase=~"Failed|Pending",namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0
      for: 5m
      labels:
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: prometheus-k8s-prometheus-rules
spec:
  groups:
  - name: prometheus
    rules:
    - alert: PrometheusBadConfig
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to
          reload its configuration.
        summary: Failed Prometheus configuration reload.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(prometheus_config_last_reload_successful{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) == 0
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusSDRefreshFailure
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to
          refresh SD with mechanism {{$labels.mechanism}}.
        summary: Failed Prometheus SD refresh.
      expr: |
        increase(prometheus_sd_refresh_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[10m]) > 0
      for: 20m
      labels:
        severity: warning
    - alert: PrometheusKubernetesListWatchFailures
      annotations:
        description: Kubernetes service discovery of Prometheus {{$labels.namespace}}/{{$labels.pod}}
          is experiencing {{ printf "%.0f" $value }} failures with LIST/WATCH requests
          to the Kubernetes API in the last 5 minutes.
        summary: Requests in Kubernetes SD are failing.
      expr: |
        increase(prometheus_sd_kubernetes_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusNotificationQueueRunningFull
      annotations:
        description: Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}}
          is running full.
        summary: Prometheus alert notification queue predicted to run full in less
          than 30m.
      expr: |
        # Without min_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          predict_linear(prometheus_notifications_queue_length{job=~"prometheus-k8s|prometheus-user-workload"}[5m], 60 * 30)
        >
          min_over_time(prometheus_notifications_queue_capacity{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
        )
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
      annotations:
        description: '{{ printf "%.1f" $value }}% errors while sending alerts from
          Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.'
        summary: Prometheus has encountered more than 1% errors sending alerts to
          a specific Alertmanager.
      expr: |
        (
          rate(prometheus_notifications_errors_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
        /
          rate(prometheus_notifications_sent_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
        )
        * 100
        > 1
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusNotConnectedToAlertmanagers
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected
          to any Alertmanagers.
        summary: Prometheus is not connected to any Alertmanagers.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(prometheus_notifications_alertmanagers_discovered{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) < 1
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusTSDBReloadsFailing
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected
          {{$value | humanize}} reload failures over the last 3h.
        summary: Prometheus has issues reloading blocks from disk.
      expr: |
        increase(prometheus_tsdb_reloads_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[3h]) > 0
      for: 4h
      labels:
        severity: warning
    - alert: PrometheusTSDBCompactionsFailing
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected
          {{$value | humanize}} compaction failures over the last 3h.
        summary: Prometheus has issues compacting blocks.
      expr: |
        increase(prometheus_tsdb_compactions_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[3h]) > 0
      for: 4h
      labels:
        severity: warning
    - alert: PrometheusNotIngestingSamples
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting
          samples.
        summary: Prometheus is not ingesting samples.
      expr: |
        (
          sum without(type) (rate(prometheus_tsdb_head_samples_appended_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m])) <= 0
        and
          (
            sum without(scrape_job) (prometheus_target_metadata_cache_entries{job=~"prometheus-k8s|prometheus-user-workload"}) > 0
          or
            sum without(rule_group) (prometheus_rule_group_rules{job=~"prometheus-k8s|prometheus-user-workload"}) > 0
          )
        )
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusDuplicateTimestamps
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping
          {{ printf "%.4g" $value  }} samples/s with different values but duplicated
          timestamp.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusDuplicateTimestamps.md
        summary: Prometheus is dropping samples with duplicate timestamps.
      expr: |
        rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 1h
      labels:
        severity: warning
    - alert: PrometheusOutOfOrderTimestamps
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping
          {{ printf "%.4g" $value  }} samples/s with timestamps arriving out of order.
        summary: Prometheus drops samples with out-of-order timestamps.
      expr: |
        rate(prometheus_target_scrapes_sample_out_of_order_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 1h
      labels:
        severity: warning
    - alert: PrometheusRemoteStorageFailures
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send
          {{ printf "%.1f" $value }}% of the samples to {{ $labels.remote_name}}:{{
          $labels.url }}
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusRemoteStorageFailures.md
        summary: Prometheus fails to send samples to remote storage.
      expr: |
        (
          (rate(prometheus_remote_storage_failed_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]))
        /
          (
            (rate(prometheus_remote_storage_failed_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]))
          +
            (rate(prometheus_remote_storage_succeeded_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) or rate(prometheus_remote_storage_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]))
          )
        )
        * 100
        > 1
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusRemoteWriteBehind
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write
          is {{ printf "%.1f" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url
          }}.
        summary: Prometheus remote write is behind.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
        - ignoring(remote_name, url) group_right
          max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
        )
        > 120
      for: 15m
      labels:
        severity: info
    - alert: PrometheusRemoteWriteDesiredShards
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write
          desired shards calculation wants to run {{ $value }} shards for queue {{
          $labels.remote_name}}:{{ $labels.url }}, which is more than the max of {{
          printf `prometheus_remote_storage_shards_max{instance="%s",job=~"prometheus-k8s|prometheus-user-workload"}`
          $labels.instance | query | first | value }}.
        summary: Prometheus remote write desired shards calculation wants to run more
          than configured max shards.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          max_over_time(prometheus_remote_storage_shards_desired{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
        >
          max_over_time(prometheus_remote_storage_shards_max{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
        )
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusRuleFailures
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to
          evaluate {{ printf "%.0f" $value }} rules in the last 5m.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusRuleFailures.md
        summary: Prometheus is failing rule evaluations.
      expr: |
        increase(prometheus_rule_evaluation_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusMissingRuleEvaluations
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{
          printf "%.0f" $value }} rule group evaluations in the last 5m.
        summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
      expr: |
        increase(prometheus_rule_group_iterations_missed_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusTargetLimitHit
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped
          {{ printf "%.0f" $value }} targets because the number of targets exceeded
          the configured target_limit.
        summary: Prometheus has dropped targets because some scrape configs have exceeded
          the targets limit.
      expr: |
        increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusLabelLimitHit
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped
          {{ printf "%.0f" $value }} targets because some samples exceeded the configured
          label_limit, label_name_length_limit or label_value_length_limit.
        summary: Prometheus has dropped targets because some scrape configs have exceeded
          the labels limit.
      expr: |
        increase(prometheus_target_scrape_pool_exceeded_label_limits_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusScrapeBodySizeLimitHit
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{
          printf "%.0f" $value }} scrapes in the last 5m because some targets exceeded
          the configured body_size_limit.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusScrapeBodySizeLimitHit.md
        summary: Prometheus has dropped some targets that exceeded body size limit.
      expr: |
        increase(prometheus_target_scrapes_exceeded_body_size_limit_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusScrapeSampleLimitHit
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{
          printf "%.0f" $value }} scrapes in the last 5m because some targets exceeded
          the configured sample_limit.
        summary: Prometheus has failed scrapes that have exceeded the configured sample
          limit.
      expr: |
        increase(prometheus_target_scrapes_exceeded_sample_limit_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusTargetSyncFailure
      annotations:
        description: '{{ printf "%.0f" $value }} targets in Prometheus {{$labels.namespace}}/{{$labels.pod}}
          have failed to sync because invalid configuration was supplied.'
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusTargetSyncFailure.md
        summary: Prometheus has failed to sync targets.
      expr: |
        increase(prometheus_target_sync_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[30m]) > 0
      for: 5m
      labels:
        severity: critical
    - alert: PrometheusHighQueryLoad
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} query API has
          less than 20% available capacity in its query engine for the last 15 minutes.
        summary: Prometheus is reaching its maximum capacity serving concurrent requests.
      expr: |
        avg_over_time(prometheus_engine_queries{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) / max_over_time(prometheus_engine_queries_concurrent_max{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0.8
      for: 15m
      labels:
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: prometheus-k8s-thanos-sidecar-rules
spec:
  groups:
  - name: thanos-sidecar
    rules:
    - alert: ThanosSidecarBucketOperationsFailed
      annotations:
        description: Thanos Sidecar {{$labels.instance}} in {{$labels.namespace}}
          bucket operations are failing
        summary: Thanos Sidecar bucket operations are failing
      expr: |
        sum by (namespace, job, instance) (rate(thanos_objstore_bucket_operation_failures_total{job=~"prometheus-(k8s|user-workload)-thanos-sidecar"}[5m])) > 0
      for: 1h
      labels:
        severity: warning
    - alert: ThanosSidecarNoConnectionToStartedPrometheus
      annotations:
        description: Thanos Sidecar {{$labels.instance}} in {{$labels.namespace}}
          is unhealthy.
        summary: Thanos Sidecar cannot access Prometheus, even though Prometheus seems
          healthy and has reloaded WAL.
      expr: |
        thanos_sidecar_prometheus_up{job=~"prometheus-(k8s|user-workload)-thanos-sidecar"} == 0
        AND on (namespace, pod)
        prometheus_tsdb_data_replay_duration_seconds != 0
      for: 1h
      labels:
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: prometheus-operator-rules
spec:
  groups:
  - name: prometheus-operator
    rules:
    - alert: PrometheusOperatorListErrors
      annotations:
        description: Errors while performing List operations in controller {{$labels.controller}}
          in {{$labels.namespace}} namespace.
        summary: Errors while performing list operations in controller.
      expr: |
        (sum by (cluster,controller,namespace) (rate(prometheus_operator_list_operations_failed_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[10m])) / sum by (cluster,controller,namespace) (rate(prometheus_operator_list_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[10m]))) > 0.4
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusOperatorWatchErrors
      annotations:
        description: Errors while performing watch operations in controller {{$labels.controller}}
          in {{$labels.namespace}} namespace.
        summary: Errors while performing watch operations in controller.
      expr: |
        (sum by (cluster,controller,namespace) (rate(prometheus_operator_watch_operations_failed_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m])) / sum by (cluster,controller,namespace) (rate(prometheus_operator_watch_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) > 0.4
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusOperatorSyncFailed
      annotations:
        description: Controller {{ $labels.controller }} in {{ $labels.namespace }}
          namespace fails to reconcile {{ $value }} objects.
        summary: Last controller reconciliation failed
      expr: |
        min_over_time(prometheus_operator_syncs{status="failed",job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) > 0
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusOperatorReconcileErrors
      annotations:
        description: '{{ $value | humanizePercentage }} of reconciling operations
          failed for {{ $labels.controller }} controller in {{ $labels.namespace }}
          namespace.'
        summary: Errors while reconciling objects.
      expr: |
        (sum by (cluster,controller,namespace) (rate(prometheus_operator_reconcile_errors_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) / (sum by (cluster,controller,namespace) (rate(prometheus_operator_reconcile_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) > 0.1
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusOperatorStatusUpdateErrors
      annotations:
        description: '{{ $value | humanizePercentage }} of status update operations
          failed for {{ $labels.controller }} controller in {{ $labels.namespace }}
          namespace.'
        summary: Errors while updating objects status.
      expr: |
        (sum by (cluster,controller,namespace) (rate(prometheus_operator_status_update_errors_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) / (sum by (cluster,controller,namespace) (rate(prometheus_operator_status_update_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) > 0.1
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusOperatorNodeLookupErrors
      annotations:
        description: Errors while reconciling Prometheus in {{ $labels.namespace }}
          Namespace.
        summary: Errors while reconciling Prometheus.
      expr: |
        rate(prometheus_operator_node_address_lookup_errors_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) > 0.1
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusOperatorNotReady
      annotations:
        description: Prometheus operator in {{ $labels.namespace }} namespace isn't
          ready to reconcile {{ $labels.controller }} resources.
        summary: Prometheus operator not ready
      expr: |
        min by (cluster,controller,namespace) (max_over_time(prometheus_operator_ready{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) == 0)
      for: 5m
      labels:
        severity: warning
    - alert: PrometheusOperatorRejectedResources
      annotations:
        description: Prometheus operator in {{ $labels.namespace }} namespace rejected
          {{ printf "%0.0f" $value }} {{ $labels.controller }}/{{ $labels.resource
          }} resources.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusOperatorRejectedResources.md
        summary: Resources rejected by Prometheus operator
      expr: |
        min_over_time(prometheus_operator_managed_resources{state="rejected",job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) > 0
      for: 5m
      labels:
        severity: warning
  - name: config-reloaders
    rules:
    - alert: ConfigReloaderSidecarErrors
      annotations:
        description: |-
          Errors encountered while the {{$labels.pod}} config-reloader sidecar attempts to sync config in {{$labels.namespace}} namespace.
          As a result, configuration for service running in {{$labels.pod}} may be stale and cannot be updated anymore.
        summary: config-reloader sidecar has not had a successful reload for 10m
      expr: |
        max_over_time(reloader_last_reload_successful{namespace=~".+"}[5m]) == 0
      for: 10m
      labels:
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: samples-operator-alerts
spec:
  groups:
  - name: SamplesOperator
    rules:
    - alert: SamplesRetriesMissingOnImagestreamImportFailing
      annotations:
        description: |
          Samples operator is detecting problems with imagestream image imports, and the periodic retries of those
          imports are not occurring.  Contact support.  You can look at the "openshift-samples" ClusterOperator object
          for details. Most likely there are issues with the external image registry hosting the images that need to
          be investigated.  The list of ImageStreams that have failing imports are:
          {{ range query "openshift_samples_failed_imagestream_import_info > 0" }}
            {{ .Labels.name }}
          {{ end }}
          However, the list of ImageStreams for which samples operator is retrying imports is:
          retrying imports:
          {{ range query "openshift_samples_retry_imagestream_import_total > 0" }}
             {{ .Labels.imagestreamname }}
          {{ end }}
        summary: Samples operator is having problems with imagestream imports and
          its retries.
      expr: sum(openshift_samples_failed_imagestream_import_info) > sum(openshift_samples_retry_imagestream_import_total)
        - sum(openshift_samples_retry_imagestream_import_total offset 30m)
      for: 2h
      labels:
        namespace: openshift-cluster-samples-operator
        severity: warning
    - alert: SamplesImagestreamImportFailing
      annotations:
        description: |
          Samples operator is detecting problems with imagestream image imports.  You can look at the "openshift-samples"
          ClusterOperator object for details. Most likely there are issues with the external image registry hosting
          the images that needs to be investigated.  Or you can consider marking samples operator Removed if you do not
          care about having sample imagestreams available.  The list of ImageStreams for which samples operator is
          retrying imports:
          {{ range query "openshift_samples_retry_imagestream_import_total > 0" }}
             {{ .Labels.imagestreamname }}
          {{ end }}
        summary: Samples operator is detecting problems with imagestream image imports
      expr: sum(openshift_samples_retry_imagestream_import_total) - sum(openshift_samples_retry_imagestream_import_total
        offset 30m) > sum(openshift_samples_failed_imagestream_import_info)
      for: 2h
      labels:
        namespace: openshift-cluster-samples-operator
        severity: warning
    - alert: SamplesDegraded
      annotations:
        description: |
          Samples could not be deployed and the operator is degraded. Review the "openshift-samples" ClusterOperator object for further details.
        summary: Samples operator is degraded.
      expr: openshift_samples_degraded_info == 1
      for: 2h
      labels:
        severity: warning
    - alert: SamplesInvalidConfig
      annotations:
        description: |
          Samples operator has been given an invalid configuration.
        summary: Samples operator Invalid configuration
      expr: openshift_samples_invalidconfig_info == 1
      for: 2h
      labels:
        severity: warning
    - alert: SamplesMissingSecret
      annotations:
        description: |
          Samples operator cannot find the samples pull secret in the openshift namespace.
        summary: Samples operator is not able to find secret
      expr: openshift_samples_invalidsecret_info{reason="missing_secret"} == 1
      for: 2h
      labels:
        severity: warning
    - alert: SamplesMissingTBRCredential
      annotations:
        description: |
          The samples operator cannot find credentials for 'registry.redhat.io'. Many of the sample ImageStreams will fail to import unless the 'samplesRegistry' in the operator configuration is changed.
        summary: Samples operator is not able to find the credentials for registry
      expr: openshift_samples_invalidsecret_info{reason="missing_tbr_credential"}
        == 1
      for: 2h
      labels:
        severity: warning
    - alert: SamplesTBRInaccessibleOnBoot
      annotations:
        description: |
          One of two situations has occurred.  Either
          samples operator could not access 'registry.redhat.io' during its initial installation and it bootstrapped as removed.
          If this is expected, and stems from installing in a restricted network environment, please note that if you
          plan on mirroring images associated with sample imagestreams into a registry available in your restricted
          network environment, and subsequently moving samples operator back to 'Managed' state, a list of the images
          associated with each image stream tag from the samples catalog is
          provided in the 'imagestreamtag-to-image' config map in the 'openshift-cluster-samples-operator' namespace to
          assist the mirroring process.
          Or, the use of allowed registries or blocked registries with global imagestream configuration will not allow
          samples operator to create imagestreams using the default image registry 'registry.redhat.io'.
        summary: Samples operator is not able to access the registry on boot
      expr: openshift_samples_tbr_inaccessible_info == 1
      for: 2d
      labels:
        severity: info
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: telemetry
spec:
  groups:
  - name: telemeter.rules
    rules:
    - expr: max(federate_samples - federate_filtered_samples)
      record: cluster:telemetry_selected_series:count
    - alert: TelemeterClientFailures
      annotations:
        description: |-
          The telemeter client in namespace {{ $labels.namespace }} fails {{ $value | humanize }} of the requests to the telemeter service.
          Check the logs of the telemeter-client pod with the following command:
          oc logs -n openshift-monitoring deployment.apps/telemeter-client -c telemeter-client
          If the telemeter client fails to authenticate with the telemeter service, make sure that the global pull secret is up to date, see https://docs.openshift.com/container-platform/latest/openshift_images/managing_images/using-image-pull-secrets.html#images-update-global-pull-secret_using-image-pull-secrets for more details.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/TelemeterClientFailures.md
        summary: Telemeter client fails to send metrics
      expr: |
        sum by (namespace) (
          rate(federate_requests_failed_total{job="telemeter-client"}[15m])
        ) /
        sum by (namespace) (
          rate(federate_requests_total{job="telemeter-client"}[15m])
        ) > 0.2
      for: 1h
      labels:
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: thanos-querier
spec:
  groups:
  - name: thanos-query
    rules:
    - alert: ThanosQueryHttpRequestQueryErrorRateHigh
      annotations:
        description: Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing
          to handle {{$value | humanize}}% of "query" requests.
        summary: Thanos Query is failing to handle requests.
      expr: |
        (
          sum by (namespace, job) (rate(http_requests_total{code=~"5..", job="thanos-querier", handler="query"}[5m]))
        /
          sum by (namespace, job) (rate(http_requests_total{job="thanos-querier", handler="query"}[5m]))
        ) * 100 > 5
      for: 1h
      labels:
        severity: warning
    - alert: ThanosQueryHttpRequestQueryRangeErrorRateHigh
      annotations:
        description: Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing
          to handle {{$value | humanize}}% of "query_range" requests.
        summary: Thanos Query is failing to handle requests.
      expr: |
        (
          sum by (namespace, job) (rate(http_requests_total{code=~"5..", job="thanos-querier", handler="query_range"}[5m]))
        /
          sum by (namespace, job) (rate(http_requests_total{job="thanos-querier", handler="query_range"}[5m]))
        ) * 100 > 5
      for: 1h
      labels:
        severity: warning
    - alert: ThanosQueryGrpcServerErrorRate
      annotations:
        description: Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing
          to handle {{$value | humanize}}% of requests.
        summary: Thanos Query is failing to handle requests.
      expr: |
        (
          sum by (namespace, job) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job="thanos-querier"}[5m]))
        /
          sum by (namespace, job) (rate(grpc_server_started_total{job="thanos-querier"}[5m]))
        * 100 > 5
        )
      for: 1h
      labels:
        severity: warning
    - alert: ThanosQueryGrpcClientErrorRate
      annotations:
        description: Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing
          to send {{$value | humanize}}% of requests.
        summary: Thanos Query is failing to send requests.
      expr: |
        (
          sum by (namespace, job) (rate(grpc_client_handled_total{grpc_code!="OK", job="thanos-querier"}[5m]))
        /
          sum by (namespace, job) (rate(grpc_client_started_total{job="thanos-querier"}[5m]))
        ) * 100 > 5
      for: 1h
      labels:
        severity: warning
    - alert: ThanosQueryHighDNSFailures
      annotations:
        description: Thanos Query {{$labels.job}} in {{$labels.namespace}} have {{$value
          | humanize}}% of failing DNS queries for store endpoints.
        summary: Thanos Query is having high number of DNS failures.
      expr: |
        (
          sum by (namespace, job) (rate(thanos_query_store_apis_dns_failures_total{job="thanos-querier"}[5m]))
        /
          sum by (namespace, job) (rate(thanos_query_store_apis_dns_lookups_total{job="thanos-querier"}[5m]))
        ) * 100 > 1
      for: 1h
      labels:
        severity: warning
    - alert: ThanosQueryOverload
      annotations:
        description: Thanos Query {{$labels.job}} in {{$labels.namespace}} has been
          overloaded for more than 15 minutes. This may be a symptom of excessive
          simultanous complex requests, low performance of the Prometheus API, or
          failures within these components. Assess the health of the Thanos query
          instances, the connnected Prometheus instances, look for potential senders
          of these requests and then contact support.
        summary: Thanos query reaches its maximum capacity serving concurrent requests.
      expr: |
        (
          max_over_time(thanos_query_concurrent_gate_queries_max[5m]) - avg_over_time(thanos_query_concurrent_gate_queries_in_flight[5m]) < 1
        )
      for: 1h
      labels:
        severity: warning
---
apiVersion: monitoring.rhobs/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    monitoring.rhobs/stack: federate-cmo-ms
  namespace: federate-cmo
  name: thanos-ruler
spec:
  groups:
  - name: thanos-rule
    rules:
    - alert: ThanosRuleQueueIsDroppingAlerts
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is
          failing to queue alerts.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/ThanosRuleQueueIsDroppingAlerts.md
        summary: Thanos Rule is failing to queue alerts.
      expr: |
        sum by (namespace, job, instance) (rate(thanos_alert_queue_alerts_dropped_total{job="thanos-ruler"}[5m])) > 0
      for: 5m
      labels:
        severity: critical
    - alert: ThanosRuleSenderIsFailingAlerts
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is
          failing to send alerts to alertmanager.
        summary: Thanos Rule is failing to send alerts to alertmanager.
      expr: |
        sum by (namespace, job, instance) (rate(thanos_alert_sender_alerts_dropped_total{job="thanos-ruler"}[5m])) > 0
      for: 5m
      labels:
        severity: warning
    - alert: ThanosRuleHighRuleEvaluationFailures
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is
          failing to evaluate rules.
        summary: Thanos Rule is failing to evaluate rules.
      expr: |
        (
          sum by (namespace, job, instance) (rate(prometheus_rule_evaluation_failures_total{job="thanos-ruler"}[5m]))
        /
          sum by (namespace, job, instance) (rate(prometheus_rule_evaluations_total{job="thanos-ruler"}[5m]))
        * 100 > 5
        )
      for: 5m
      labels:
        severity: warning
    - alert: ThanosRuleHighRuleEvaluationWarnings
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has
          high number of evaluation warnings.
        summary: Thanos Rule has high number of evaluation warnings.
      expr: |
        sum by (namespace, job, instance) (rate(thanos_rule_evaluation_with_warnings_total{job="thanos-ruler"}[5m])) > 0
      for: 15m
      labels:
        severity: info
    - alert: ThanosRuleRuleEvaluationLatencyHigh
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has
          higher evaluation latency than interval for {{$labels.rule_group}}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/ThanosRuleRuleEvaluationLatencyHigh.md
        summary: Thanos Rule has high rule evaluation latency.
      expr: |
        (
          sum by (namespace, job, instance, rule_group) (prometheus_rule_group_last_duration_seconds{job="thanos-ruler"})
        >
          sum by (namespace, job, instance, rule_group) (prometheus_rule_group_interval_seconds{job="thanos-ruler"})
        )
      for: 5m
      labels:
        severity: warning
    - alert: ThanosRuleGrpcErrorRate
      annotations:
        description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} is failing
          to handle {{$value | humanize}}% of requests.
        summary: Thanos Rule is failing to handle grpc requests.
      expr: |
        (
          sum by (namespace, job, instance) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job="thanos-ruler"}[5m]))
        /
          sum by (namespace, job, instance) (rate(grpc_server_started_total{job="thanos-ruler"}[5m]))
        * 100 > 5
        )
      for: 5m
      labels:
        severity: warning
    - alert: ThanosRuleConfigReloadFailure
      annotations:
        description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has not
          been able to reload its configuration.
        summary: Thanos Rule has not been able to reload configuration.
      expr: avg by (namespace, job, instance) (thanos_rule_config_last_reload_successful{job="thanos-ruler"})
        != 1
      for: 5m
      labels:
        severity: info
    - alert: ThanosRuleQueryHighDNSFailures
      annotations:
        description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has {{$value
          | humanize}}% of failing DNS queries for query endpoints.
        summary: Thanos Rule is having high number of DNS failures.
      expr: |
        (
          sum by (namespace, job, instance) (rate(thanos_rule_query_apis_dns_failures_total{job="thanos-ruler"}[5m]))
        /
          sum by (namespace, job, instance) (rate(thanos_rule_query_apis_dns_lookups_total{job="thanos-ruler"}[5m]))
        * 100 > 1
        )
      for: 15m
      labels:
        severity: warning
    - alert: ThanosRuleAlertmanagerHighDNSFailures
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has
          {{$value | humanize}}% of failing DNS queries for Alertmanager endpoints.
        summary: Thanos Rule is having high number of DNS failures.
      expr: |
        (
          sum by (namespace, job, instance) (rate(thanos_rule_alertmanagers_dns_failures_total{job="thanos-ruler"}[5m]))
        /
          sum by (namespace, job, instance) (rate(thanos_rule_alertmanagers_dns_lookups_total{job="thanos-ruler"}[5m]))
        * 100 > 1
        )
      for: 15m
      labels:
        severity: warning
    - alert: ThanosRuleNoEvaluationFor10Intervals
      annotations:
        description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has rule
          groups that did not evaluate for at least 10x of their expected interval.
        summary: Thanos Rule has rule groups that did not evaluate for 10 intervals.
      expr: |
        time() -  max by (namespace, job, instance, group) (prometheus_rule_group_last_evaluation_timestamp_seconds{job="thanos-ruler"})
        >
        10 * max by (namespace, job, instance, group) (prometheus_rule_group_interval_seconds{job="thanos-ruler"})
      for: 5m
      labels:
        severity: info
    - alert: ThanosNoRuleEvaluations
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} did
          not perform any rule evaluations in the past 10 minutes.
        summary: Thanos Rule did not perform any rule evaluations.
      expr: |
        sum by (namespace, job, instance) (rate(prometheus_rule_evaluations_total{job="thanos-ruler"}[5m])) <= 0
          and
        sum by (namespace, job, instance) (thanos_rule_loaded_rules{job="thanos-ruler"}) > 0
      for: 5m
      labels:
        severity: warning
